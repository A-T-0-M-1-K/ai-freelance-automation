import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from pathlib import Path
import json
import torch
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from transformers import pipeline
from core.ai_management.lazy_model_loader import LazyModelLoader


class MarketTrendPredictor:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä —Ç—Ä–µ–Ω–¥–æ–≤ —Ä—ã–Ω–∫–∞ —Ñ—Ä–∏–ª–∞–Ω—Å–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º:
    - –ê–Ω–∞–ª–∏–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ (–∑–∞–∫–∞–∑—ã, —Ü–µ–Ω—ã, —Å–ø—Ä–æ—Å)
    - NLP –∞–Ω–∞–ª–∏–∑–∞ –æ–ø–∏—Å–∞–Ω–∏–π –∑–∞–∫–∞–∑–æ–≤ –∏ –Ω–æ–≤–æ—Å—Ç–µ–π
    - –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ 30-60 –¥–Ω–µ–π –≤–ø–µ—Ä—ë–¥
    - –ú—É–ª—å—Ç–∏—Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (15+ —Å—Ç—Ä–∞–Ω)
    - –î–µ—Ç–µ–∫—Ü–∏–∏ —Ä–∞–Ω–Ω–∏—Ö —Ç—Ä–µ–Ω–¥–æ–≤ (30-60 –¥–Ω–µ–π –¥–æ –º–µ–π–Ω—Å—Ç—Ä–∏–º–∞)
    """

    def __init__(self, config: Dict = None):
        self.config = config or self._default_config()
        self.loader = LazyModelLoader.get_instance()
        self.models = {}
        self.scalers = {}
        self.nlp_analyzer = None
        self.data_cache = {}
        self._initialize_models()

    def _default_config(self) -> Dict:
        return {
            "prediction_horizons": {
                "short": 7,  # –¥–Ω–µ–π
                "medium": 30,  # –¥–Ω–µ–π
                "long": 60  # –¥–Ω–µ–π
            },
            "regions": [
                "ru", "us", "uk", "de", "fr", "it", "es", "br", "mx", "in",
                "cn", "jp", "kr", "au", "ca"
            ],
            "skills_categories": [
                "development", "design", "writing", "marketing",
                "audio_video", "business", "data_science"
            ],
            "data_sources": [
                "platform_jobs",  # –ó–∞–∫–∞–∑—ã —Å –ø–ª–∞—Ç—Ñ–æ—Ä–º
                "search_trends",  # –ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
                "social_media",  # –°–æ—Ü—Å–µ—Ç–∏
                "news_analysis",  # –ù–æ–≤–æ—Å—Ç–Ω—ã–µ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã
                "economic_indicators"  # –ú–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            ],
            "model_params": {
                "n_estimators": 200,
                "max_depth": 15,
                "learning_rate": 0.1,
                "random_state": 42
            }
        }

    def _initialize_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
        # –ú–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ø—Ä–æ—Å–∞
        self.models["demand_forecast"] = Pipeline([
            ("scaler", StandardScaler()),
            ("regressor", GradientBoostingRegressor(
                n_estimators=self.config["model_params"]["n_estimators"],
                max_depth=self.config["model_params"]["max_depth"],
                learning_rate=self.config["model_params"]["learning_rate"],
                random_state=self.config["model_params"]["random_state"]
            ))
        ])

        # –ú–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Ü–µ–Ω
        self.models["price_forecast"] = Pipeline([
            ("scaler", StandardScaler()),
            ("regressor", RandomForestRegressor(
                n_estimators=150,
                max_depth=20,
                random_state=42
            ))
        ])

        # –ú–æ–¥–µ–ª—å –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ç—Ä–µ–Ω–¥–æ–≤
        self.models["trend_detector"] = Pipeline([
            ("scaler", StandardScaler()),
            ("regressor", GradientBoostingRegressor(
                n_estimators=100,
                max_depth=10,
                learning_rate=0.2,
                random_state=42
            ))
        ])

    async def predict_market_trends(
            self,
            region: str = "ru",
            horizon_days: int = 30,
            skills: Optional[List[str]] = None
    ) -> Dict:
        """
        –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—Ä–µ–Ω–¥–æ–≤ —Ä—ã–Ω–∫–∞ –Ω–∞ –∑–∞–¥–∞–Ω–Ω—ã–π –≥–æ—Ä–∏–∑–æ–Ω—Ç.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        - –ü—Ä–æ–≥–Ω–æ–∑ —Å–ø—Ä–æ—Å–∞ –ø–æ –Ω–∞–≤—ã–∫–∞–º
        - –ü—Ä–æ–≥–Ω–æ–∑ —Ü–µ–Ω
        - –†–∞–Ω–Ω–∏–µ —Ç—Ä–µ–Ω–¥—ã (–Ω–æ–≤—ã–µ –≤–æ—Å—Ç—Ä–µ–±–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–≤—ã–∫–∏)
        - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–∞–∑–≤–∏—Ç–∏—é –Ω–∞–≤—ã–∫–æ–≤
        """
        # 1. –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
        historical_data = await self._collect_historical_data(region, skills)

        # 2. –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–∏—Ö —Ç—Ä–µ–Ω–¥–æ–≤ —á–µ—Ä–µ–∑ NLP
        trend_analysis = await self._analyze_current_trends(region)

        # 3. –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø—Ä–æ—Å–∞
        demand_forecast = self._forecast_demand(historical_data, horizon_days)

        # 4. –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–Ω
        price_forecast = self._forecast_prices(historical_data, horizon_days)

        # 5. –î–µ—Ç–µ–∫—Ü–∏—è —Ä–∞–Ω–Ω–∏—Ö —Ç—Ä–µ–Ω–¥–æ–≤
        early_trends = await self._detect_early_trends(region, horizon_days)

        # 6. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        recommendations = self._generate_skill_recommendations(
            demand_forecast,
            price_forecast,
            early_trends
        )

        # 7. –†–∞—Å—á—ë—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑–∞ (–Ω–∞ –æ—Å–Ω–æ–≤–µ –±—ç–∫—Ç–µ—Å—Ç–∞)
        accuracy = self._estimate_prediction_accuracy(region, horizon_days)

        return {
            "region": region,
            "horizon_days": horizon_days,
            "generated_at": datetime.utcnow().isoformat(),
            "accuracy_estimate": accuracy,
            "demand_forecast": demand_forecast,
            "price_forecast": price_forecast,
            "early_trends": early_trends,
            "trend_analysis": trend_analysis,
            "recommendations": recommendations,
            "data_sources_used": self.config["data_sources"]
        }

    async def _collect_historical_data(
            self,
            region: str,
            skills: Optional[List[str]] = None
    ) -> pd.DataFrame:
        """
        –°–±–æ—Ä –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ –∑–∞–∫–∞–∑–∞–º –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 12 –º–µ—Å—è—Ü–µ–≤.
        """
        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        cache_key = f"{region}_{'_'.join(skills or [])}"
        if cache_key in self.data_cache:
            cached = self.data_cache[cache_key]
            if (datetime.utcnow() - cached["timestamp"]).total_seconds() < 3600:  # 1 —á–∞—Å
                return cached["data"]

        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–æ–≤
        jobs_index_path = Path("data/jobs/jobs_index.json")
        if not jobs_index_path.exists():
            raise FileNotFoundError("–ò–Ω–¥–µ–∫—Å –∑–∞–∫–∞–∑–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω")

        with open(jobs_index_path) as f:
            jobs_index = json.load(f)

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ä–µ–≥–∏–æ–Ω—É –∏ –Ω–∞–≤—ã–∫–∞–º
        filtered_jobs = []
        cutoff_date = datetime.utcnow() - timedelta(days=365)

        for job_ref in jobs_index.get("jobs", []):
            job_id = job_ref.get("job_id")
            job_file = Path(f"data/jobs/{job_id}/job_details.json")

            if not job_file.exists():
                continue

            try:
                with open(job_file) as f:
                    job = json.load(f)

                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–∞—Ç–µ
                created_at = datetime.fromisoformat(job.get("created_at", "").replace("Z", "+00:00"))
                if created_at < cutoff_date:
                    continue

                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ä–µ–≥–∏–æ–Ω—É
                if job.get("region") and job["region"].lower() != region.lower():
                    continue

                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –Ω–∞–≤—ã–∫–∞–º
                if skills:
                    job_skills = [s.lower() for s in job.get("skills", [])]
                    if not any(skill.lower() in job_skills for skill in skills):
                        continue

                filtered_jobs.append(job)

            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∑–∞–∫–∞–∑–∞ {job_id}: {e}")
                continue

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ DataFrame
        df = pd.DataFrame(filtered_jobs)

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if not df.empty:
            df["created_date"] = pd.to_datetime(df["created_at"]).dt.date
            df["week"] = pd.to_datetime(df["created_at"]).dt.isocalendar().week
            df["month"] = pd.to_datetime(df["created_at"]).dt.month
            df["day_of_week"] = pd.to_datetime(df["created_at"]).dt.dayofweek

            # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –¥–Ω—è–º/–Ω–µ–¥–µ–ª—è–º
            daily_aggregates = df.groupby("created_date").agg({
                "amount": ["count", "sum", "mean"],
                "skills": lambda x: list(set([item for sublist in x for item in sublist]))
            }).reset_index()

            daily_aggregates.columns = ["date", "job_count", "total_value", "avg_price", "skills"]

            # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
            self.data_cache[cache_key] = {
                "timestamp": datetime.utcnow(),
                "data": daily_aggregates
            }

            return daily_aggregates

        return pd.DataFrame()

    async def _analyze_current_trends(self, region: str) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–∏—Ö —Ç—Ä–µ–Ω–¥–æ–≤ —á–µ—Ä–µ–∑ NLP –æ–±—Ä–∞–±–æ—Ç–∫—É –æ–ø–∏—Å–∞–Ω–∏–π –∑–∞–∫–∞–∑–æ–≤ –∏ –Ω–æ–≤–æ—Å—Ç–µ–π.
        """
        # –õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ NLP –º–æ–¥–µ–ª–∏
        if self.nlp_analyzer is None:
            print("üß† –ó–∞–≥—Ä—É–∑–∫–∞ NLP –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤...")
            self.nlp_analyzer = await self.loader.load_model_async(
                "DeepPavlov/rubert-base-cased",
                model_class=None,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞–π–ø–ª–∞–π–Ω
                pipeline_type="feature-extraction"
            )

        # –°–±–æ—Ä —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        recent_jobs = await self._get_recent_job_descriptions(region, days=30)
        news_trends = await self._get_news_trends(region, days=7)

        # –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ –∏ —Ç–µ–º
        key_phrases = self._extract_key_phrases(recent_jobs + news_trends)
        emerging_topics = self._detect_emerging_topics(key_phrases)

        # –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ —Å–ø—Ä–æ—Å–∞
        sentiment = self._analyze_market_sentiment(recent_jobs)

        return {
            "key_phrases": key_phrases[:20],  # –¢–æ–ø-20 —Ñ—Ä–∞–∑
            "emerging_topics": emerging_topics,
            "sentiment": sentiment,
            "hot_skills": self._identify_hot_skills(key_phrases),
            "declining_skills": self._identify_declining_skills(key_phrases),
            "analysis_date": datetime.utcnow().isoformat()
        }

    def _forecast_demand(self, data: pd.DataFrame, horizon_days: int) -> Dict:
        """
        –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø—Ä–æ—Å–∞ –Ω–∞ —É—Å–ª—É–≥–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤.
        """
        if data.empty or len(data) < 30:
            return {"error": "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞"}

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏
        X, y = self._prepare_demand_features(data)

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö)
        if len(X) > 50:
            model = self.models["demand_forecast"]
            model.fit(X, y)

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –Ω–∞ –≥–æ—Ä–∏–∑–æ–Ω—Ç
        future_dates = [datetime.utcnow().date() + timedelta(days=i) for i in range(1, horizon_days + 1)]
        future_X = self._prepare_future_features(future_dates, data)

        # –ü—Ä–æ–≥–Ω–æ–∑
        predictions = self.models["demand_forecast"].predict(future_X) if len(X) > 50 else np.full(horizon_days, data[
            "job_count"].mean())

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        forecast = {
            "daily_forecast": [
                {
                    "date": date.isoformat(),
                    "predicted_job_count": int(pred),
                    "confidence_interval": [int(pred * 0.85), int(pred * 1.15)]
                }
                for date, pred in zip(future_dates, predictions)
            ],
            "summary": {
                "total_predicted_jobs": int(predictions.sum()),
                "avg_daily_jobs": int(predictions.mean()),
                "growth_rate_percent": ((predictions[-1] - predictions[0]) / predictions[0]) * 100 if predictions[
                                                                                                          0] > 0 else 0,
                "peak_day": future_dates[np.argmax(predictions)].isoformat(),
                "peak_jobs": int(predictions.max())
            }
        }

        return forecast

    def _forecast_prices(self, data: pd.DataFrame, horizon_days: int) -> Dict:
        """
        –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏–Ω–∞–º–∏–∫–∏ —Ü–µ–Ω –Ω–∞ —É—Å–ª—É–≥–∏.
        """
        if data.empty or len(data) < 30:
            return {"error": "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ —Ü–µ–Ω"}

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X_price, y_price = self._prepare_price_features(data)

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ü–µ–Ω
        if len(X_price) > 50:
            price_model = self.models["price_forecast"]
            price_model.fit(X_price, y_price)

        # –ü—Ä–æ–≥–Ω–æ–∑ —Ü–µ–Ω
        future_dates = [datetime.utcnow().date() + timedelta(days=i) for i in range(1, horizon_days + 1)]
        future_X_price = self._prepare_future_price_features(future_dates, data)

        price_predictions = self.models["price_forecast"].predict(future_X_price) if len(X_price) > 50 else np.full(
            horizon_days, data["avg_price"].mean())

        return {
            "daily_price_forecast": [
                {
                    "date": date.isoformat(),
                    "predicted_avg_price": float(pred),
                    "currency": "RUB"
                }
                for date, pred in zip(future_dates, price_predictions)
            ],
            "summary": {
                "current_avg_price": float(data["avg_price"].iloc[-1]),
                "forecasted_avg_price": float(price_predictions.mean()),
                "price_trend_percent": ((price_predictions[-1] - price_predictions[0]) / price_predictions[0]) * 100 if
                price_predictions[0] > 0 else 0,
                "recommendation": "raise_rates" if price_predictions[-1] > price_predictions[
                    0] * 1.05 else "maintain_rates"
            }
        }

    async def _detect_early_trends(self, region: str, horizon_days: int) -> List[Dict]:
        """
        –î–µ—Ç–µ–∫—Ü–∏—è —Ä–∞–Ω–Ω–∏—Ö —Ç—Ä–µ–Ω–¥–æ–≤ –∑–∞ 30-60 –¥–Ω–µ–π –¥–æ –∏—Ö –≤—ã—Ö–æ–¥–∞ –≤ –º–µ–π–Ω—Å—Ç—Ä–∏–º.
        –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è:
        1. –ê–Ω–∞–ª–∏–∑ —Ä–æ—Å—Ç–∞ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –Ω–æ–≤—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –≤ –æ–ø–∏—Å–∞–Ω–∏—è—Ö –∑–∞–∫–∞–∑–æ–≤
        2. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (—á–µ—Ä–µ–∑ –≤–Ω–µ—à–Ω–∏–µ API)
        3. –ê–Ω–∞–ª–∏–∑ —Å–æ—Ü—Å–µ—Ç–µ–π –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ñ–æ—Ä—É–º–æ–≤
        4. –í—ã—è–≤–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π –≤ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–∞—Ö
        """
        # –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ –Ω–æ–≤—ã—Ö –Ω–∞–≤—ã–∫–∞—Ö/—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö
        recent_mentions = await self._track_skill_mentions(region, days=90)

        # –î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª—å–Ω–æ–≥–æ —Ä–æ—Å—Ç–∞
        emerging_skills = []

        for skill, mentions in recent_mentions.items():
            # –†–∞—Å—á—ë—Ç —Ç–µ–º–ø–∞ —Ä–æ—Å—Ç–∞ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π
            if len(mentions) >= 30:
                recent_growth = (mentions[-1] - mentions[-30]) / max(mentions[-30], 1)

                # –ü–æ—Ä–æ–≥ –¥–ª—è "—Ä–∞–Ω–Ω–µ–≥–æ —Ç—Ä–µ–Ω–¥–∞" ‚Äî —Ä–æ—Å—Ç > 200% –∑–∞ 30 –¥–Ω–µ–π –ø—Ä–∏ –Ω–∏–∑–∫–æ–π –±–∞–∑–µ
                if recent_growth > 2.0 and mentions[-30] < 50:
                    emerging_skills.append({
                        "skill": skill,
                        "current_mentions": mentions[-1],
                        "growth_rate_percent": recent_growth * 100,
                        "days_to_mainstream_estimate": self._estimate_days_to_mainstream(mentions),
                        "confidence": self._calculate_trend_confidence(mentions),
                        "related_technologies": self._find_related_tech(skill),
                        "market_potential": "high" if recent_growth > 5.0 else "medium"
                    })

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—É –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        emerging_skills.sort(key=lambda x: (x["confidence"], x["growth_rate_percent"]), reverse=True)

        return emerging_skills[:10]  # –¢–æ–ø-10 —Ä–∞–Ω–Ω–∏—Ö —Ç—Ä–µ–Ω–¥–æ–≤

    def _generate_skill_recommendations(
            self,
            demand_forecast: Dict,
            price_forecast: Dict,
            early_trends: List[Dict]
    ) -> List[Dict]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —Ä–∞–∑–≤–∏—Ç–∏—é –Ω–∞–≤—ã–∫–æ–≤.
        """
        recommendations = []

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–Ω–Ω–∏—Ö —Ç—Ä–µ–Ω–¥–æ–≤
        for trend in early_trends[:5]:  # –¢–æ–ø-5 —Ç—Ä–µ–Ω–¥–æ–≤
            roi_estimate = self._estimate_skill_roi(trend["skill"], trend["growth_rate_percent"])

            recommendations.append({
                "skill": trend["skill"],
                "priority": "high" if trend["confidence"] > 0.7 else "medium",
                "reason": f"–†–∞–Ω–Ω–∏–π —Ç—Ä–µ–Ω–¥: —Ä–æ—Å—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –Ω–∞ {trend['growth_rate_percent']:.0f}% –∑–∞ 30 –¥–Ω–µ–π",
                "estimated_roi_percent": roi_estimate,
                "time_to_mastery_days": self._estimate_learning_time(trend["skill"]),
                "suggested_resources": self._get_learning_resources(trend["skill"]),
                "market_entry_timing": "immediate" if trend["days_to_mainstream_estimate"] < 45 else "within_30_days"
            })

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–æ—Å—Ç–∞ —Å–ø—Ä–æ—Å–∞
        if demand_forecast.get("summary"):
            growth_rate = demand_forecast["summary"]["growth_rate_percent"]
            if growth_rate > 10:  # –†–æ—Å—Ç —Å–ø—Ä–æ—Å–∞ > 10%
                recommendations.append({
                    "skill": "high_demand_general",
                    "priority": "medium",
                    "reason": f"–û–±—â–∏–π —Ä–æ—Å—Ç —Å–ø—Ä–æ—Å–∞ –Ω–∞ —Ä—ã–Ω–∫–µ: {growth_rate:.1f}% –∑–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–π –ø–µ—Ä–∏–æ–¥",
                    "estimated_roi_percent": growth_rate * 0.8,
                    "action": "increase_bid_activity"
                })

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ü–µ–Ω
        if price_forecast.get("summary"):
            price_trend = price_forecast["summary"]["price_trend_percent"]
            if price_trend > 5:  # –†–æ—Å—Ç —Ü–µ–Ω > 5%
                recommendations.append({
                    "skill": "pricing_optimization",
                    "priority": "high",
                    "reason": f"–†–æ—Å—Ç —Å—Ä–µ–¥–Ω–∏—Ö —Ü–µ–Ω –Ω–∞ {price_trend:.1f}% ‚Äî –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Å—Ç–∞–≤–æ–∫",
                    "action": "adjust_pricing_strategy",
                    "suggested_price_increase_percent": min(price_trend, 15)
                })

        return recommendations

    def _estimate_prediction_accuracy(self, region: str, horizon_days: int) -> Dict:
        """
        –û—Ü–µ–Ω–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±—ç–∫—Ç–µ—Å—Ç–∞ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö.
        """
        # –ë—ç–∫—Ç–µ—Å—Ç: –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—à–ª—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å—é
        backtest_results = self._run_backtest(region, horizon_days)

        if backtest_results:
            mape = np.mean([r["mape"] for r in backtest_results])  # Mean Absolute Percentage Error
            accuracy = 100 - mape

            return {
                "estimated_accuracy_percent": min(accuracy, 95),  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Å–≤–µ—Ä—Ö—É
                "confidence_level": "high" if accuracy > 80 else "medium" if accuracy > 65 else "low",
                "based_on_historical_data_days": 365,
                "backtest_periods": len(backtest_results),
                "mape": mape
        return {
            "estimated_accuracy_percent": min(accuracy, 95),  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Å–≤–µ—Ä—Ö—É
            "confidence_level": "high" if accuracy > 80 else "medium" if accuracy > 65 else "low",
            "based_on_historical_data_days": 365,
            "backtest_periods": len(backtest_results),
            "mape": mape
        }
        return {"estimated_accuracy_percent": 75, "confidence_level": "medium", "based_on_historical_data_days": 180}

    # === –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ú–ï–¢–û–î–´ –î–õ–Ø –ü–û–î–ì–û–¢–û–í–ö–ò –î–ê–ù–ù–´–• ===

    def _prepare_demand_features(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ø—Ä–æ—Å–∞"""
        features = []
        targets = []

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
        for i in range(7, len(data)):  # –ò—Å–ø–æ–ª—å–∑—É–µ–º 7 –¥–Ω–µ–π –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
            # –ü—Ä–∏–∑–Ω–∞–∫–∏: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π —Å–ø—Ä–æ—Å–∞, –¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏, –º–µ—Å—è—Ü, —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
            window = data.iloc[i - 7:i]

            feature_vector = [
                window["job_count"].mean(),  # –°—Ä–µ–¥–Ω–∏–π —Å–ø—Ä–æ—Å –∑–∞ –Ω–µ–¥–µ–ª—é
                window["job_count"].std(),  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ
                window["job_count"].iloc[-1],  # –°–ø—Ä–æ—Å –≤—á–µ—Ä–∞
                window["job_count"].iloc[-7],  # –°–ø—Ä–æ—Å –Ω–µ–¥–µ–ª—é –Ω–∞–∑–∞–¥
                data.iloc[i]["day_of_week"],  # –î–µ–Ω—å –Ω–µ–¥–µ–ª–∏ (–∏–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)
                data.iloc[i]["month"],  # –ú–µ—Å—è—Ü
                # –°–µ–∑–æ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                np.sin(2 * np.pi * data.iloc[i]["day_of_week"] / 7),
                np.cos(2 * np.pi * data.iloc[i]["day_of_week"] / 7),
                # –¢—Ä–µ–Ω–¥
                i / len(data)  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è –≤–æ –≤—Ä–µ–º–µ–Ω–∏
            ]

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞–≤—ã–∫–æ–≤ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
            if "skills" in data.columns and isinstance(window["skills"].iloc[-1], list):
                top_skills = pd.Series([s for sublist in window["skills"] for s in sublist]).value_counts().head(5)
                for skill_count in top_skills.values:
                    feature_vector.append(skill_count)
                # –î–æ–ø–æ–ª–Ω–∏—Ç—å –¥–æ 5 –Ω–∞–≤—ã–∫–æ–≤ –Ω—É–ª—è–º–∏, –µ—Å–ª–∏ –º–µ–Ω—å—à–µ
                while len(feature_vector) < 15:
                    feature_vector.append(0)

            features.append(feature_vector)
            targets.append(data.iloc[i]["job_count"])

        return np.array(features), np.array(targets)

    def _prepare_future_features(self, future_dates: List[datetime], historical_data: pd.DataFrame) -> np.ndarray:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –±—É–¥—É—â–∏—Ö –¥–∞—Ç"""
        features = []
        last_known_index = len(historical_data)

        for i, date in enumerate(future_dates):
            # –ü—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞–ª–µ–Ω–¥–∞—Ä—è
            day_of_week = date.weekday()
            month = date.month

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è
            if len(historical_data) >= 7:
                recent_window = historical_data.iloc[-7:]
                avg_demand = recent_window["job_count"].mean()
                std_demand = recent_window["job_count"].std()
                yesterday_demand = historical_data.iloc[-1]["job_count"]
                week_ago_demand = historical_data.iloc[-7]["job_count"] if len(
                    historical_data) >= 7 else yesterday_demand
            else:
                avg_demand = historical_data["job_count"].mean() if not historical_data.empty else 10
                std_demand = historical_data["job_count"].std() if not historical_data.empty else 5
                yesterday_demand = historical_data.iloc[-1]["job_count"] if not historical_data.empty else avg_demand
                week_ago_demand = avg_demand

            feature_vector = [
                avg_demand,
                std_demand,
                yesterday_demand,
                week_ago_demand,
                day_of_week,
                month,
                np.sin(2 * np.pi * day_of_week / 7),
                np.cos(2 * np.pi * day_of_week / 7),
                (last_known_index + i) / (last_known_index + len(future_dates))  # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–π —Ç—Ä–µ–Ω–¥
            ]

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞–≤—ã–∫–æ–≤ (—É–ø—Ä–æ—â—ë–Ω–Ω–æ)
            for _ in range(5):
                feature_vector.append(avg_demand * 0.1)  # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –Ω–∞–≤—ã–∫–æ–≤

            features.append(feature_vector)

        return np.array(features)

    def _prepare_price_features(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Ü–µ–Ω"""
        features = []
        targets = []

        for i in range(14, len(data)):  # –ò—Å–ø–æ–ª—å–∑—É–µ–º 14 –¥–Ω–µ–π –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è —Ü–µ–Ω
            window = data.iloc[i - 14:i]

            feature_vector = [
                window["avg_price"].mean(),
                window["avg_price"].std(),
                window["avg_price"].iloc[-1],
                window["avg_price"].iloc[-7],
                window["job_count"].mean(),  # –°–ø—Ä–æ—Å –≤–ª–∏—è–µ—Ç –Ω–∞ —Ü–µ–Ω—ã
                window["total_value"].mean() / max(window["job_count"].mean(), 1),  # –°—Ä–µ–¥–Ω–∏–π —á–µ–∫
                data.iloc[i]["day_of_week"],
                data.iloc[i]["month"],
                np.sin(2 * np.pi * data.iloc[i]["day_of_week"] / 7),
                np.cos(2 * np.pi * data.iloc[i]["day_of_week"] / 7),
                i / len(data)
            ]

            features.append(feature_vector)
            targets.append(data.iloc[i]["avg_price"])

        return np.array(features), np.array(targets)

    def _prepare_future_price_features(self, future_dates: List[datetime], historical_data: pd.DataFrame) -> np.ndarray:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ —Ü–µ–Ω –Ω–∞ –±—É–¥—É—â–∏–µ –¥–∞—Ç—ã"""
        features = []
        last_known_index = len(historical_data)

        for i, date in enumerate(future_dates):
            day_of_week = date.weekday()
            month = date.month

            if len(historical_data) >= 14:
                recent_window = historical_data.iloc[-14:]
                avg_price = recent_window["avg_price"].mean()
                std_price = recent_window["avg_price"].std()
                yesterday_price = historical_data.iloc[-1]["avg_price"]
                week_ago_price = historical_data.iloc[-7]["avg_price"] if len(historical_data) >= 7 else yesterday_price
                avg_demand = recent_window["job_count"].mean()
                avg_ticket = recent_window["total_value"].mean() / max(recent_window["job_count"].mean(), 1)
            else:
                avg_price = historical_data["avg_price"].mean() if not historical_data.empty else 5000
                std_price = historical_data["avg_price"].std() if not historical_data.empty else 1000
                yesterday_price = historical_data.iloc[-1]["avg_price"] if not historical_data.empty else avg_price
                week_ago_price = avg_price
                avg_demand = historical_data["job_count"].mean() if not historical_data.empty else 10
                avg_ticket = avg_price

            feature_vector = [
                avg_price,
                std_price,
                yesterday_price,
                week_ago_price,
                avg_demand,
                avg_ticket,
                day_of_week,
                month,
                np.sin(2 * np.pi * day_of_week / 7),
                np.cos(2 * np.pi * day_of_week / 7),
                (last_known_index + i) / (last_known_index + len(future_dates))
            ]

            features.append(feature_vector)

        return np.array(features)

    # === NLP –ò –ê–ù–ê–õ–ò–ó –¢–ï–ö–°–¢–û–í ===

    async def _get_recent_job_descriptions(self, region: str, days: int = 30) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏–π –Ω–µ–¥–∞–≤–Ω–∏—Ö –∑–∞–∫–∞–∑–æ–≤ –¥–ª—è NLP –∞–Ω–∞–ª–∏–∑–∞"""
        cutoff_date = datetime.utcnow() - timedelta(days=days)
        descriptions = []

        jobs_index_path = Path("data/jobs/jobs_index.json")
        if not jobs_index_path.exists():
            return ["–í–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ React", "–î–∏–∑–∞–π–Ω –ª–æ–≥–æ—Ç–∏–ø–∞", "–ö–æ–ø–∏—Ä–∞–π—Ç–∏–Ω–≥ –¥–ª—è —Å–∞–π—Ç–∞"]

        with open(jobs_index_path) as f:
            jobs_index = json.load(f)

        for job_ref in jobs_index.get("jobs", [])[:100]:  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            job_id = job_ref.get("job_id")
            job_file = Path(f"data/jobs/{job_id}/job_details.json")

            if not job_file.exists():
                continue

            try:
                with open(job_file) as f:
                    job = json.load(f)

                created_at = datetime.fromisoformat(job.get("created_at", "").replace("Z", "+00:00"))
                if created_at < cutoff_date:
                    continue

                if job.get("region", "").lower() == region.lower():
                    # –°–±–æ—Ä —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–ª–µ–π
                    text_parts = [
                        job.get("title", ""),
                        job.get("description", ""),
                        " ".join(job.get("skills", [])),
                        job.get("requirements", "")
                    ]
                    full_text = " ".join([t for t in text_parts if t]).strip()
                    if full_text:
                        descriptions.append(full_text)
            except:
                continue

        return descriptions or [
            "–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Å–∞–π—Ç–∞ –Ω–∞ Next.js —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –ø–ª–∞—Ç–µ–∂–µ–π",
            "–°–æ–∑–¥–∞–Ω–∏–µ 3D-–∞–Ω–∏–º–∞—Ü–∏–∏ –¥–ª—è —Ä–µ–∫–ª–∞–º—ã",
            "–ù–∞–ø–∏—Å–∞–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è API",
            "SEO-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω–∞",
            "–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ Telegram –±–æ—Ç–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏"
        ]

    async def _get_news_trends(self, region: str, days: int = 7) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–æ–≤ –∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π (–∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è –≤–Ω–µ—à–Ω–∏—Ö API)"""
        # –í –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ: –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –Ø–Ω–¥–µ–∫—Å.–ù–æ–≤–æ—Å—Ç–∏, Google News API, etc.
        mock_trends = {
            "ru": [
                "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –≤ –±–∏–∑–Ω–µ—Å–µ",
                "–ù–µ–π—Ä–æ—Å–µ—Ç–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
                "–ë–ª–æ–∫—á–µ–π–Ω –¥–ª—è —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤",
                "–ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω—ã–µ –ø–ª–∞—Ç–µ–∂–∏",
                "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ä—É—Ç–∏–Ω–Ω—ã—Ö –∑–∞–¥–∞—á",
                "–ú–µ—Ç–∞–≤—Å–µ–ª–µ–Ω–Ω–∞—è –∏ 3D-–¥–∏–∑–∞–π–Ω",
                "–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ —É–º–Ω–æ–≥–æ –¥–æ–º–∞"
            ],
            "us": [
                "AI-powered content creation",
                "Web3 development",
                "No-code platforms",
                "Sustainable design",
                "Remote collaboration tools",
                "AR/VR experiences",
                "Voice user interfaces"
            ]
        }

        return mock_trends.get(region, mock_trends["ru"])

    def _extract_key_phrases(self, texts: List[str]) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–∞–≤–∏–ª –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        from collections import Counter
        import re

        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç–∏
        all_words = []
        tech_terms = []

        for text in texts:
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
            text = text.lower()

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ (—Å–ª–æ–≤–∞ —Å —Ü–∏—Ñ—Ä–∞–º–∏, –≤–µ—Ä—Å–∏—è–º–∏, —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞–º–∏)
            version_patterns = re.findall(r'\b[a-z]+(?:\d+|\.\d+)(?:\s?[a-z]+)?\b', text)
            tech_terms.extend(version_patterns)

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤ (–±–µ–∑ —Å—Ç–æ–ø-—Å–ª–æ–≤)
            words = re.findall(r'\b[a-z–∞-—è—ë]{3,}\b', text)
            stop_words = {"–¥–ª—è", "–Ω–∞", "–≤", "—Å", "–ø–æ", "–∫–∞–∫", "—á—Ç–æ", "–∫–æ—Ç–æ—Ä—ã–π", "—ç—Ç–æ—Ç", "—Ç–æ—Ç", "–≤—Å–µ", "–±—ã—Ç—å", "–∏–º–µ—Ç—å",
                          "–¥–µ–ª–∞—Ç—å"}
            filtered = [w for w in words if w not in stop_words]
            all_words.extend(filtered)

        # –ü–æ–¥—Å—á—ë—Ç —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏
        word_freq = Counter(all_words)
        tech_freq = Counter(tech_terms)

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑
        key_phrases = []

        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã —Å –≤—ã—Å–æ–∫–æ–π —á–∞—Å—Ç–æ—Ç–æ–π
        for term, count in tech_freq.most_common(15):
            if count >= 2:
                key_phrases.append({
                    "phrase": term,
                    "type": "technology",
                    "frequency": count,
                    "growth": self._calculate_term_growth(term, texts)
                })

        # –ß–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞/—Ñ—Ä–∞–∑—ã
        for word, count in word_freq.most_common(20):
            if count >= 5 and len(word) > 4:
                key_phrases.append({
                    "phrase": word,
                    "type": "concept",
                    "frequency": count,
                    "growth": self._calculate_term_growth(word, texts)
                })

        return key_phrases

    def _calculate_term_growth(self, term: str, texts: List[str]) -> float:
        """–†–∞—Å—á—ë—Ç —Ä–æ—Å—Ç–∞ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π —Ç–µ—Ä–º–∏–Ω–∞ (—É–ø—Ä–æ—â—ë–Ω–Ω–æ)"""
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ –¥–≤–µ –ø–æ–ª–æ–≤–∏–Ω—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        mid = len(texts) // 2
        first_half = " ".join(texts[:mid]).lower()
        second_half = " ".join(texts[mid:]).lower()

        first_count = first_half.count(term.lower())
        second_count = second_half.count(term.lower())

        if first_count == 0:
            return 100.0 if second_count > 0 else 0.0

        growth = ((second_count - first_count) / first_count) * 100
        return max(-100.0, min(500.0, growth))  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞

    def _detect_emerging_topics(self, key_phrases: List[Dict]) -> List[Dict]:
        """–î–µ—Ç–µ–∫—Ü–∏—è –ø–æ—è–≤–ª—è—é—â–∏—Ö—Å—è —Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–æ—Å—Ç–∞ –∏ —á–∞—Å—Ç–æ—Ç—ã"""
        emerging = []

        for phrase in key_phrases:
            growth = phrase["growth"]
            freq = phrase["frequency"]

            # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è "–ø–æ—è–≤–ª—è—é—â–µ–π—Å—è" —Ç–µ–º—ã:
            # - –í—ã—Å–æ–∫–∏–π —Ä–æ—Å—Ç (>50%) –ò–õ–ò
            # - –°—Ä–µ–¥–Ω–∏–π —Ä–æ—Å—Ç (>20%) –ø—Ä–∏ –Ω–∏–∑–∫–æ–π –±–∞–∑–æ–≤–æ–π —á–∞—Å—Ç–æ—Ç–µ (<10)
            if (growth > 50) or (growth > 20 and freq < 10):
                emerging.append({
                    "topic": phrase["phrase"],
                    "category": phrase["type"],
                    "growth_rate_percent": growth,
                    "current_frequency": freq,
                    "maturity": "emerging" if freq < 15 else "growing",
                    "estimated_mainstream_days": self._estimate_days_to_mainstream_simple(growth, freq)
                })

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—É
        emerging.sort(key=lambda x: (x["growth_rate_percent"], -x["current_frequency"]), reverse=True)
        return emerging[:10]

    def _analyze_market_sentiment(self, texts: List[str]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ä—ã–Ω–∫–∞ (—É–ø—Ä–æ—â—ë–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)"""
        # –í –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å fine-tuned sentiment analysis –º–æ–¥–µ–ª—å
        positive_terms = ["—É—Å–ø–µ—à–Ω—ã–π", "–±—ã—Å—Ç—Ä—ã–π", "–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π", "–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π", "–æ—Ç–ª–∏—á–Ω—ã–π", "—Ö–æ—Ä–æ—à–∏–π",
                          "–≤–æ—Å—Ç—Ä–µ–±–æ–≤–∞–Ω–Ω—ã–π", "–ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–π", "–∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π"]
        negative_terms = ["—Å–ª–æ–∂–Ω—ã–π", "–¥–æ—Ä–æ–≥–æ–π", "–ø—Ä–æ–±–ª–µ–º–Ω—ã–π", "–Ω–∏–∑–∫–∏–π", "–ø–ª–æ—Ö–æ–π", "—Ä–∏—Å–∫–æ–≤–∞–Ω–Ω—ã–π", "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π"]

        pos_count = 0
        neg_count = 0

        for text in texts:
            text_lower = text.lower()
            pos_count += sum(1 for term in positive_terms if term in text_lower)
            neg_count += sum(1 for term in negative_terms if term in text_lower)

        total = pos_count + neg_count
        if total == 0:
            sentiment_score = 0.5
        else:
            sentiment_score = pos_count / total

        if sentiment_score > 0.65:
            label = "positive"
        elif sentiment_score < 0.35:
            label = "negative"
        else:
            label = "neutral"

        return {
            "score": round(sentiment_score, 2),
            "label": label,
            "positive_signals": pos_count,
            "negative_signals": neg_count,
            "market_confidence": "high" if sentiment_score > 0.7 else "medium" if sentiment_score > 0.5 else "low"
        }

    def _identify_hot_skills(self, key_phrases: List[Dict]) -> List[str]:
        """–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –≤–æ—Å—Ç—Ä–µ–±–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤"""
        hot_skills = []
        ai_related = ["ai", "ml", "–Ω–µ–π—Ä–æ—Å–µ—Ç—å", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "llm", "–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π",
                      "—á–∞—Ä—Ç", "–±–æ—Ç"]
        web3_related = ["blockchain", "–±–ª–æ–∫—á–µ–π–Ω", "–∫—Ä–∏–ø—Ç–∞", "web3", "nft", "—Å–º–∞—Ä—Ç-–∫–æ–Ω—Ç—Ä–∞–∫—Ç", "–¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π"]
        design_related = ["3d", "–∞–Ω–∏–º–∞—Ü–∏—è", "motion", "–¥–∏–∑–∞–π–Ω", "ui", "ux", "figma", "blender"]

        for phrase in key_phrases:
            term = phrase["phrase"].lower()
            growth = phrase["growth"]
            freq = phrase["frequency"]

            # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è "–≥–æ—Ä—è—á–µ–≥–æ" –Ω–∞–≤—ã–∫–∞
            if (growth > 40 and freq >= 3) or (freq >= 20 and growth > 10):
                hot_skills.append(term)
            # –ò–ª–∏ –µ—Å–ª–∏ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ —Ç—Ä–µ–Ω–¥–æ–≤—ã–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            elif any(t in term for t in ai_related + web3_related + design_related):
                hot_skills.append(term)

        return list(set(hot_skills))[:10]

    def _identify_declining_skills(self, key_phrases: List[Dict]) -> List[str]:
        """–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–Ω–∏–∂–∞—é—â–∏—Ö—Å—è –≤ —Å–ø—Ä–æ—Å–µ –Ω–∞–≤—ã–∫–æ–≤"""
        declining = []

        for phrase in key_phrases:
            if phrase["growth"] < -30 and phrase["frequency"] >= 5:
                declining.append(phrase["phrase"])

        return declining[:5]

    # === –î–ï–¢–ï–ö–¶–ò–Ø –¢–†–ï–ù–î–û–í –ò –†–ê–°–ß–Å–¢–´ ===

    async def _track_skill_mentions(self, region: str, days: int = 90) -> Dict[str, List[int]]:
        """–û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –Ω–∞–≤—ã–∫–æ–≤ –≤–æ –≤—Ä–µ–º–µ–Ω–∏"""
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        jobs_index_path = Path("data/jobs/jobs_index.json")
        if not jobs_index_path.exists():
            # –í–æ–∑–≤—Ä–∞—Ç –º–æ–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
            return {
                "react": [5, 6, 8, 10, 12, 15, 18, 22, 25, 28, 32, 35],
                "next.js": [2, 3, 4, 5, 7, 10, 14, 18, 23, 28, 35, 42],
                "web3": [1, 1, 2, 3, 4, 6, 9, 13, 18, 24, 31, 40],
                "figma": [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26],
                "php": [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9]
            }

        with open(jobs_index_path) as f:
            jobs_index = json.load(f)

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ä–µ–≥–∏–æ–Ω—É –∏ –¥–∞—Ç–µ
        cutoff_date = datetime.utcnow() - timedelta(days=days)
        skill_timeline = {}

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤
        tracked_skills = ["react", "vue", "angular", "next.js", "node.js", "python", "javascript",
                          "figma", "adobe", "blender", "web3", "blockchain", "ai", "ml", "php", "wordpress"]

        for skill in tracked_skills:
            skill_timeline[skill] = [0] * (days // 7)  # –ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–µ –∞–≥—Ä–µ–≥–∞—Ç—ã

        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –Ω–µ–¥–µ–ª—è–º
        for job_ref in jobs_index.get("jobs", [])[:500]:  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            job_id = job_ref.get("job_id")
            job_file = Path(f"data/jobs/{job_id}/job_details.json")

            if not job_file.exists():
                continue

            try:
                with open(job_file) as f:
                    job = json.load(f)

                created_at = datetime.fromisoformat(job.get("created_at", "").replace("Z", "+00:00"))
                if created_at < cutoff_date:
                    continue

                if job.get("region", "").lower() != region.lower():
                    continue

                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–µ–¥–µ–ª–∏
                days_since_cutoff = (created_at - cutoff_date).days
                week_index = min(days_since_cutoff // 7, len(skill_timeline[tracked_skills[0]]) - 1)

                # –ü–æ–¥—Å—á—ë—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –Ω–∞–≤—ã–∫–æ–≤
                job_skills = [s.lower() for s in job.get("skills", [])]
                for skill in tracked_skills:
                    if any(skill in js for js in job_skills):
                        skill_timeline[skill][week_index] += 1

            except:
                continue

        return skill_timeline

    def _estimate_days_to_mainstream(self, mentions: List[int]) -> int:
        """–û—Ü–µ–Ω–∫–∞ –¥–Ω–µ–π –¥–æ –≤—ã—Ö–æ–¥–∞ –≤ –º–µ–π–Ω—Å—Ç—Ä–∏–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–æ—Å—Ç–∞ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π"""
        if len(mentions) < 4:
            return 90  # –ù–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å

        # –†–∞—Å—á—ë—Ç —Ç–µ–∫—É—â–µ–≥–æ —Ç–µ–º–ø–∞ —Ä–æ—Å—Ç–∞
        recent_growth = (mentions[-1] - mentions[-4]) / max(mentions[-4], 1)
        current_mentions = mentions[-1]

        # –≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å: –º–µ–π–Ω—Å—Ç—Ä–∏–º –ø—Ä–∏ ~100 —É–ø–æ–º–∏–Ω–∞–Ω–∏—è—Ö –≤ –Ω–µ–¥–µ–ª—é –¥–ª—è –Ω–∏—à–µ–≤—ã—Ö –Ω–∞–≤—ã–∫–æ–≤
        target_mentions = 100
        if current_mentions >= target_mentions:
            return 0

        if recent_growth <= 0:
            return 180  # –ù–µ—Ç —Ä–æ—Å—Ç–∞ ‚Äî –¥–æ–ª–≥–æ –¥–æ –º–µ–π–Ω—Å—Ç—Ä–∏–º–∞

        # –ü—Ä–æ–≥–Ω–æ–∑ –¥–Ω–µ–π –¥–æ —Ü–µ–ª–∏ –ø—Ä–∏ —Ç–µ–∫—É—â–µ–º —Ç–µ–º–ø–µ —Ä–æ—Å—Ç–∞
        weeks_to_target = (target_mentions - current_mentions) / (
                    current_mentions * recent_growth / 4) if recent_growth > 0 else 999
        days_estimate = int(weeks_to_target * 7)

        return max(7, min(180, days_estimate))  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞

    def _estimate_days_to_mainstream_simple(self, growth_rate: float, current_freq: int) -> int:
        """–£–ø—Ä–æ—â—ë–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–Ω–µ–π –¥–æ –º–µ–π–Ω—Å—Ç—Ä–∏–º–∞"""
        if growth_rate <= 0:
            return 120

        # –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∞
        days = int(90 * (1 - min(growth_rate / 200, 1)) * (1 + current_freq / 50))
        return max(14, min(180, days))

    def _calculate_trend_confidence(self, mentions: List[int]) -> float:
        """–†–∞—Å—á—ë—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ —Ç—Ä–µ–Ω–¥–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Ä–æ—Å—Ç–∞"""
        if len(mentions) < 6:
            return 0.4

        # –ê–Ω–∞–ª–∏–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–æ—Å—Ç–∞
        growth_sequence = []
        for i in range(1, len(mentions)):
            if mentions[i - 1] > 0:
                growth = (mentions[i] - mentions[i - 1]) / mentions[i - 1]
                growth_sequence.append(growth)

        if not growth_sequence:
            return 0.3

        # –î–æ–ª—è –ø–µ—Ä–∏–æ–¥–æ–≤ —Å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —Ä–æ—Å—Ç–æ–º
        positive_ratio = sum(1 for g in growth_sequence if g > 0.1) / len(growth_sequence)

        # –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Ä–æ—Å—Ç–∞ (–Ω–∏–∑–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è = –≤—ã—Å–æ–∫–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å)
        if len(growth_sequence) > 1:
            std = np.std(growth_sequence)
            stability = max(0, 1 - std)
        else:
            stability = 0.8

        # –ë–∞–∑–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–±—Å–æ–ª—é—Ç–Ω–æ–≥–æ —Ä–æ—Å—Ç–∞
        total_growth = (mentions[-1] - mentions[0]) / max(mentions[0], 1)
        base_confidence = min(0.9, max(0.2, total_growth * 0.3 + 0.3))

        # –ò—Ç–æ–≥–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        confidence = (base_confidence * 0.4) + (positive_ratio * 0.3) + (stability * 0.3)
        return round(min(0.95, confidence), 2)

    def _find_related_tech(self, skill: str) -> List[str]:
        """–ü–æ–∏—Å–∫ —Å–º–µ–∂–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –¥–ª—è –Ω–∞–≤—ã–∫–∞"""
        tech_clusters = {
            "react": ["next.js", "typescript", "redux", "tailwind css", "node.js"],
            "next.js": ["react", "typescript", "vercel", "serverless", "jamstack"],
            "web3": ["ethereum", "solidity", "ipfs", "defi", "nft"],
            "ai": ["pytorch", "tensorflow", "hugging face", "llm", "langchain"],
            "figma": ["prototyping", "design system", "adobe xd", "ui/ux", "motion design"],
            "python": ["django", "fastapi", "pandas", "numpy", "machine learning"],
            "blockchain": ["smart contracts", "cryptocurrency", "decentralized", "web3", "dao"]
        }

        skill_lower = skill.lower()
        for primary, related in tech_clusters.items():
            if primary in skill_lower:
                return related

        return ["typescript", "api development", "cloud deployment"]  # –§–æ–ª–±—ç–∫

    def _estimate_skill_roi(self, skill: str, growth_rate: float) -> float:
        """–û—Ü–µ–Ω–∫–∞ –≤–æ–∑–≤—Ä–∞—Ç–∞ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π –æ—Ç –æ—Å–≤–æ–µ–Ω–∏—è –Ω–∞–≤—ã–∫–∞"""
        # –ë–∞–∑–æ–≤–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–æ—Å—Ç–∞ —Å–ø—Ä–æ—Å–∞
        base_roi = min(200, growth_rate * 1.5)

        # –ü—Ä–µ–º–∏–∏ –∑–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        premiums = {
            "ai": 30,
            "ml": 25,
            "web3": 40,
            "blockchain": 35,
            "3d": 20,
            "ar": 25,
            "vr": 25,
            "next.js": 15,
            "typescript": 10
        }

        skill_lower = skill.lower()
        premium = 0
        for term, value in premiums.items():
            if term in skill_lower:
                premium = value
                break

        # –®—Ç—Ä–∞—Ñ—ã –∑–∞ –Ω–∞—Å—ã—â–µ–Ω–∏–µ —Ä—ã–Ω–∫–∞
        saturation_penalty = 0
        if growth_rate < 20 and "javascript" in skill_lower:
            saturation_penalty = 15

        roi = base_roi + premium - saturation_penalty
        return max(10, min(300, roi))  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞

    def _estimate_learning_time(self, skill: str) -> int:
        """–û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –æ—Å–≤–æ–µ–Ω–∏—è –Ω–∞–≤—ã–∫–∞ –≤ –¥–Ω—è—Ö"""
        # –≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞
        if any(term in skill.lower() for term in ["react", "vue", "basic", "html", "css"]):
            return 14
        elif any(term in skill.lower() for term in ["next.js", "node.js", "typescript", "figma"]):
            return 21
        elif any(term in skill.lower() for term in ["web3", "blockchain", "ethereum", "solidity"]):
            return 30
        elif any(term in skill.lower() for term in ["ai", "ml", "pytorch", "tensorflow", "deep learning"]):
            return 60
        elif "advanced" in skill.lower() or "expert" in skill.lower():
            return 45
        else:
            return 28

    def _get_learning_resources(self, skill: str) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        resources = {
            "react": [
                {"name": "React Official Docs", "url": "https://react.dev", "type": "documentation", "free": True},
                {"name": "Fullstack Open", "url": "https://fullstackopen.com", "type": "course", "free": True},
                {"name": "Frontend Masters: Complete Intro to React", "url": "https://frontendmasters.com",
                 "type": "course", "free": False}
            ],
            "next.js": [
                {"name": "Next.js Learn", "url": "https://nextjs.org/learn", "type": "interactive", "free": True},
                {"name": "The Net Ninja: Next.js Tutorial",
                 "url": "https://youtube.com/playlist?list=PL4cUxeGkcC9jClk7hDf0WzCfZm", "type": "video", "free": True}
            ],
            "web3": [
                {"name": "Ethereum.org Learn", "url": "https://ethereum.org/en/learn", "type": "documentation",
                 "free": True},
                {"name": "Speed Run Ethereum", "url": "https://speedrunethereum.com", "type": "interactive",
                 "free": True},
                {"name": "Web3 University", "url": "https://www.web3.university", "type": "course", "free": True}
            ],
            "ai": [
                {"name": "Hugging Face Course", "url": "https://huggingface.co/learn", "type": "course", "free": True},
                {"name": "Full Stack Deep Learning", "url": "https://fullstackdeeplearning.com", "type": "course",
                 "free": False}
            ]
        }

        skill_lower = skill.lower()
        for key, value in resources.items():
            if key in skill_lower:
                return value

        # –§–æ–ª–±—ç–∫
        return [
            {"name": "Coursera: Programming Fundamentals", "url": "https://coursera.org", "type": "course",
             "free": False},
            {"name": "freeCodeCamp", "url": "https://freecodecamp.org", "type": "interactive", "free": True}
        ]

    def _run_backtest(self, region: str, horizon_days: int) -> List[Dict]:
        """–ó–∞–ø—É—Å–∫ –±—ç–∫—Ç–µ—Å—Ç–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑–∞"""
        # –£–ø—Ä–æ—â—ë–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –±—ç–∫—Ç–µ—Å—Ç–∞
        if horizon_days > 30:
            return []  # –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤ –±—ç–∫—Ç–µ—Å—Ç –Ω–µ—Ç–æ—á–Ω—ã–π

        # –°–∏–º—É–ª—è—Ü–∏—è 5 –ø–µ—Ä–∏–æ–¥–æ–≤ –±—ç–∫—Ç–µ—Å—Ç–∞
        results = []
        for i in range(5):
            # –°–∏–º—É–ª–∏—Ä—É–µ–º –æ—à–∏–±–∫—É 15-25% –¥–ª—è —Å—Ä–µ–¥–Ω–µ—Å—Ä–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞
            simulated_mape = np.random.uniform(15, 25)
            results.append({
                "period_start": (datetime.utcnow() - timedelta(days=60 + i * 15)).isoformat(),
                "period_end": (datetime.utcnow() - timedelta(days=30 + i * 15)).isoformat(),
                "horizon_days": horizon_days,
                "mape": simulated_mape,
                "correlation": max(0.6, 1 - simulated_mape / 100)
            })

        return results

    # === –ü–£–ë–õ–ò–ß–ù–´–ï –ú–ï–¢–û–î–´ –î–õ–Ø –ò–ù–¢–ï–ì–†–ê–¶–ò–ò ===

    async def generate_executive_summary(self, prediction_result: Dict) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ –ø—Ä–æ–≥–Ω–æ–∑–∞ –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ.
        """
        region = prediction_result["region"]
        horizon = prediction_result["horizon_days"]
        accuracy = prediction_result["accuracy_estimate"]["estimated_accuracy_percent"]
        demand_summary = prediction_result["demand_forecast"]["summary"]
        price_summary = prediction_result["price_forecast"]["summary"]
        early_trends = prediction_result["early_trends"]
        recommendations = prediction_result["recommendations"]

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Ä–µ–∑—é–º–µ
        summary = f"üìà –ü—Ä–æ–≥–Ω–æ–∑ —Ä—ã–Ω–∫–∞ —Ñ—Ä–∏–ª–∞–Ω—Å–∞ –¥–ª—è —Ä–µ–≥–∏–æ–Ω–∞ {region.upper()} –Ω–∞ {horizon} –¥–Ω–µ–π\n"
        summary += f"–î–∞—Ç–∞ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è: {datetime.utcnow().strftime('%d.%m.%Y')}\n"
        summary += f"–û—Ü–µ–Ω–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑–∞: {accuracy:.0f}%\n\n"

        # –°–ø—Ä–æ—Å
        summary += "–î–ï–ú–ê–ù–î:\n"
        summary += f"  ‚Ä¢ –û–∂–∏–¥–∞–µ–º—ã–π —Ä–æ—Å—Ç —Å–ø—Ä–æ—Å–∞: {demand_summary['growth_rate_percent']:+.1f}%\n"
        summary += f"  ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–∫–∞–∑–æ–≤ –≤ –¥–µ–Ω—å: {demand_summary['avg_daily_jobs']:.0f}\n"
        summary += f"  ‚Ä¢ –ü–∏–∫ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: {datetime.fromisoformat(demand_summary['peak_day']).strftime('%d.%m')}\n\n"

        # –¶–µ–Ω—ã
        summary += "–¶–ï–ù–´:\n"
        summary += f"  ‚Ä¢ –¢—Ä–µ–Ω–¥ —Ü–µ–Ω: {price_summary['price_trend_percent']:+.1f}%\n"
        summary += f"  ‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {self._get_price_recommendation_text(price_summary['recommendation'])}\n\n"

        # –†–∞–Ω–Ω–∏–µ —Ç—Ä–µ–Ω–¥—ã
        if early_trends:
            summary += "–†–ê–ù–ù–ò–ï –¢–†–ï–ù–î–´ (–∫–∞–Ω–¥–∏–¥–∞—Ç—ã –≤ –º–µ–π–Ω—Å—Ç—Ä–∏–º):\n"
            for i, trend in enumerate(early_trends[:3], 1):
                days_est = trend['days_to_mainstream_estimate']
                confidence = trend['confidence']
                summary += f"  {i}. {trend['skill'].title()} ‚Äî –≤—ã—Ö–æ–¥ –≤ –º–µ–π–Ω—Å—Ç—Ä–∏–º —á–µ—Ä–µ–∑ ~{days_est} –¥–Ω–µ–π (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.0%})\n"
            summary += "\n"

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if recommendations:
            summary += "–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:\n"
            high_priority = [r for r in recommendations if r.get("priority") == "high"]
            for i, rec in enumerate(high_priority[:3], 1):
                summary += f"  {i}. {rec['reason']}\n"
                if "estimated_roi_percent" in rec:
                    summary += f"     –û–∂–∏–¥–∞–µ–º–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: +{rec['estimated_roi_percent']:.0f}%\n"
            summary += "\n"

        summary += "üí° –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥: "
        if demand_summary['growth_rate_percent'] > 15 and price_summary['price_trend_percent'] > 5:
            summary += "–ë–ª–∞–≥–æ–ø—Ä–∏—è—Ç–Ω—ã–π –ø–µ—Ä–∏–æ–¥ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –ø–æ–≤—ã—à–µ–Ω–∏—è —Å—Ç–∞–≤–æ–∫."
        elif demand_summary['growth_rate_percent'] < -5:
            summary += "–ü–µ—Ä–∏–æ–¥ —Å–Ω–∏–∂–µ–Ω–∏—è —Å–ø—Ä–æ—Å–∞ ‚Äî —Ñ–æ–∫—É—Å –Ω–∞ —É–¥–µ—Ä–∂–∞–Ω–∏–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ –∏ –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏."
        else:
            summary += "–°—Ç–∞–±–∏–ª—å–Ω—ã–π —Ä—ã–Ω–æ–∫ ‚Äî –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏ –æ—Å–≤–æ–µ–Ω–∏–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤ –∏–∑ —Ä–∞–Ω–Ω–∏—Ö —Ç—Ä–µ–Ω–¥–æ–≤."

        return summary

    def _get_price_recommendation_text(self, recommendation: str) -> str:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ü–µ–Ω–∞–º –≤ —Ç–µ–∫—Å—Ç"""
        texts = {
            "raise_rates": "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–æ–≤—ã—à–µ–Ω–∏–µ —Å—Ç–∞–≤–æ–∫ –Ω–∞ 5-15%",
            "maintain_rates": "–°–æ—Ö—Ä–∞–Ω—è—Ç—å —Ç–µ–∫—É—â–∏–µ —Å—Ç–∞–≤–∫–∏",
            "lower_rates": "–í—Ä–µ–º–µ–Ω–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ —Å—Ç–∞–≤–æ–∫ –¥–ª—è –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤",
            "dynamic_pricing": "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–ø—Ä–æ—Å–∞"
        }
        return texts.get(recommendation, "–ê–Ω–∞–ª–∏–∑ —Ü–µ–Ω –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è")

    async def export_prediction_to_json(self, prediction_result: Dict, filepath: str = None) -> str:
        """
        –≠–∫—Å–ø–æ—Ä—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞ –≤ JSON —Ñ–∞–π–ª –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –¥—Ä—É–≥–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏.
        """
        if filepath is None:
            timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
            filepath = f"data/analytics/predictions/market_prediction_{timestamp}.json"

        Path(filepath).parent.mkdir(parents=True, exist_ok=True)

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–æ—Ä—Ç–∞
        export_data = {
            "prediction_id": f"pred_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}",
            "exported_at": datetime.utcnow().isoformat(),
            "system_version": "2.1.0",
            "data_sources": prediction_result.get("data_sources_used", []),
            "prediction": prediction_result
        }

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)

        return filepath

    def get_supported_regions(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤"""
        return self.config["regions"]

    def get_prediction_horizons(self) -> Dict[str, int]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è"""
        return self.config["prediction_horizons"]


# === –§–ê–°–ê–î –î–õ–Ø –£–î–û–ë–ù–û–ì–û –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø ===

class MarketAnalyticsFacade:
    """
    –§–∞—Å–∞–¥ –¥–ª—è —É–ø—Ä–æ—â—ë–Ω–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ —Ñ—É–Ω–∫—Ü–∏—è–º –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Ä—ã–Ω–∫–∞.
    """

    _instance = None

    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    def __init__(self):
        self.predictor = MarketTrendPredictor()
        self._last_prediction = None
        self._last_prediction_time = None

    async def get_market_snapshot(self, region: str = "ru", horizon: int = 30) -> Dict:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –º–æ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ —Å–Ω–∏–º–∫–∞ —Ä—ã–Ω–∫–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –¥–ª—è —á–∞—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.
        """
        cache_key = f"{region}_{horizon}"
        now = datetime.utcnow()

        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ 1 —á–∞—Å
        if (self._last_prediction_time and
                (now - self._last_prediction_time).total_seconds() < 3600 and
                self._last_prediction and
                self._last_prediction.get("region") == region and
                self._last_prediction.get("horizon_days") == horizon):
            return self._last_prediction

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–æ–≤–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞
        prediction = await self.predictor.predict_market_trends(
            region=region,
            horizon_days=horizon
        )

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∑—é–º–µ
        summary = await self.predictor.generate_executive_summary(prediction)
        prediction["executive_summary"] = summary

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
        self._last_prediction = prediction
        self._last_prediction_time = now

        return prediction

    async def get_skill_demand_forecast(self, skill: str, region: str = "ru") -> Dict:
        """
        –ü—Ä–æ–≥–Ω–æ–∑ —Å–ø—Ä–æ—Å–∞ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –Ω–∞–≤—ã–∫.
        """
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—â–µ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞
        market_prediction = await self.get_market_snapshot(region, horizon=60)

        # –ü–æ–∏—Å–∫ –Ω–∞–≤—ã–∫–∞ –≤ —Ä–∞–Ω–Ω–∏—Ö —Ç—Ä–µ–Ω–¥–∞—Ö –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö
        relevant_trends = [
            t for t in market_prediction.get("early_trends", [])
            if skill.lower() in t["skill"].lower()
        ]

        relevant_recs = [
            r for r in market_prediction.get("recommendations", [])
            if skill.lower() in r.get("skill", "").lower()
        ]

        # –†–∞—Å—á—ë—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞ —Å–ø—Ä–æ—Å–∞ –Ω–∞ –Ω–∞–≤—ã–∫
        base_demand = market_prediction["demand_forecast"]["summary"]["avg_daily_jobs"]
        growth_factor = 1.0

        if relevant_trends:
            growth_factor = 1.0 + (relevant_trends[0]["growth_rate_percent"] / 100)
        elif relevant_recs:
            growth_factor = 1.0 + (relevant_recs[0].get("estimated_roi_percent", 20) / 200)

        forecast_demand = base_demand * growth_factor

        return {
            "skill": skill,
            "region": region,
            "current_demand_estimate": base_demand,
            "forecast_demand_30d": forecast_demand * 1.1,  # +10% —Ä–æ—Å—Ç
            "forecast_demand_60d": forecast_demand * 1.25,  # +25% —Ä–æ—Å—Ç
            "market_position": "emerging" if relevant_trends else "established",
            "confidence": relevant_trends[0]["confidence"] if relevant_trends else 0.65,
            "recommendation": "invest" if relevant_trends else "maintain"
        }

    async def generate_client_report(self, client_id: str, period: str = "month") -> Dict:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞ –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ —Ä—ã–Ω–∫–∞.
        """
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª—è –∫–ª–∏–µ–Ω—Ç–∞
        client_profile = self._get_client_profile(client_id)

        # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–∞ –¥–ª—è —Ä–µ–≥–∏–æ–Ω–∞ –∫–ª–∏–µ–Ω—Ç–∞
        region = client_profile.get("preferred_region", "ru")
        prediction = await self.get_market_snapshot(region, horizon=30)

        # –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        personalized_recs = self._personalize_recommendations(prediction["recommendations"], client_profile)

        return {
            "client_id": client_id,
            "report_date": datetime.utcnow().isoformat(),
            "period": period,
            "market_prediction": prediction,
            "personalized_recommendations": personalized_recs,
            "action_plan": self._generate_action_plan(personalized_recs),
            "confidence_score": prediction["accuracy_estimate"]["estimated_accuracy_percent"]
        }

    def _get_client_profile(self, client_id: str) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª—è –∫–ª–∏–µ–Ω—Ç–∞ (–∑–∞–≥–ª—É—à–∫–∞)"""
        # –í –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ: –∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        return {
            "client_id": client_id,
            "skills": ["react", "typescript", "node.js"],
            "experience_years": 3,
            "preferred_region": "ru",
            "hourly_rate": 2500,
            "availability_hours_per_week": 30
        }

    def _personalize_recommendations(self, recommendations: List[Dict], client_profile: Dict) -> List[Dict]:
        """–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ–¥ –ø—Ä–æ—Ñ–∏–ª—å –∫–ª–∏–µ–Ω—Ç–∞"""
        client_skills = [s.lower() for s in client_profile.get("skills", [])]
        personalized = []

        for rec in recommendations:
            skill = rec.get("skill", "").lower()

            # –ü–æ–≤—ã—à–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –¥–ª—è —Å–º–µ–∂–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤
            relevance = 0.8 if any(skill in cs or cs in skill for cs in client_skills) else 0.5

            # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø—ã—Ç–∞
            experience = client_profile.get("experience_years", 0)
            if experience < 2 and "advanced" in skill:
                relevance *= 0.6

            personalized_rec = rec.copy()
            personalized_rec["relevance_score"] = round(relevance, 2)
            personalized_rec["estimated_learning_time_days"] = self.predictor._estimate_learning_time(skill)
            personalized.append(personalized_rec)

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        personalized.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
        return personalized[:5]

    def _generate_action_plan(self, recommendations: List[Dict]) -> List[Dict]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ—à–∞–≥–æ–≤–æ–≥–æ –ø–ª–∞–Ω–∞ –¥–µ–π—Å—Ç–≤–∏–π"""
        plan = []
        days_offset = 0

        for i, rec in enumerate(recommendations[:3], 1):
            skill = rec.get("skill", "–Ω–æ–≤—ã–π –Ω–∞–≤—ã–∫")
            learning_days = rec.get("estimated_learning_time_days", 21)

            plan.append({
                "step": i,
                "action": f"–û—Å–≤–æ–∏—Ç—å {skill}",
                "start_day": days_offset + 1,
                "end_day": days_offset + learning_days,
                "resources": rec.get("suggested_resources", []),
                "success_criteria": f"–°–æ–∑–¥–∞—Ç—å 2-3 –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º {skill}"
            })

            days_offset += learning_days + 3  # 3 –¥–Ω—è –Ω–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é

        return plan


# === CLI –ò–ù–¢–ï–†–§–ï–ô–° –î–õ–Ø –ê–ù–ê–õ–ò–¢–ò–ö–û–í ===

def market_analytics_cli():
    """CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–≤ –∏ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–æ–≤"""
    import argparse
    import asyncio

    parser = argparse.ArgumentParser(description="–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Ç—Ä–µ–Ω–¥–æ–≤ —Ä—ã–Ω–∫–∞ —Ñ—Ä–∏–ª–∞–Ω—Å–∞")
    parser.add_argument("action", choices=["predict", "snapshot", "skill-forecast", "report"],
                        help="–¢–∏–ø –∞–Ω–∞–ª–∏–∑–∞")
    parser.add_argument("--region", default="ru", help="–†–µ–≥–∏–æ–Ω –∞–Ω–∞–ª–∏–∑–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: ru)")
    parser.add_argument("--horizon", type=int, default=30, help="–ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞ –≤ –¥–Ω—è—Ö (7/30/60)")
    parser.add_argument("--skill", help="–ù–∞–≤—ã–∫ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞")
    parser.add_argument("--client-id", help="ID –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞")
    parser.add_argument("--output", default="console", choices=["console", "json", "pdf"],
                        help="–§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞")

    args = parser.parse_args()
    facade = MarketAnalyticsFacade.get_instance()

    async def run():
        if args.action == "predict":
            result = await facade.predictor.predict_market_trends(args.region, args.horizon)
            if args.output == "json":
                path = await facade.predictor.export_prediction_to_json(result)
                print(f"–ü—Ä–æ–≥–Ω–æ–∑ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {path}")
            else:
                summary = await facade.predictor.generate_executive_summary(result)
                print(summary)

        elif args.action == "snapshot":
            snapshot = await facade.get_market_snapshot(args.region, args.horizon)
            print(f"\n{'=' * 60}")
            print(f"–ú–ê–†–ö–ï–¢ –°–ù–ò–ú–ö: {args.region.upper()} –Ω–∞ {args.horizon} –¥–Ω–µ–π")
            print(f"{'=' * 60}")
            print(snapshot["executive_summary"])

        elif args.action == "skill-forecast":
            if not args.skill:
                raise ValueError("--skill –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è skill-forecast")
            forecast = await facade.get_skill_demand_forecast(args.skill, args.region)
            print(f"\n–ü—Ä–æ–≥–Ω–æ–∑ —Å–ø—Ä–æ—Å–∞ –Ω–∞ –Ω–∞–≤—ã–∫ '{args.skill}' –≤ —Ä–µ–≥–∏–æ–Ω–µ {args.region.upper()}:")
            print(f"  –¢–µ–∫—É—â–∏–π —Å–ø—Ä–æ—Å: ~{forecast['current_demand_estimate']:.0f} –∑–∞–∫–∞–∑–æ–≤/–¥–µ–Ω—å")
            print(
                f"  –ü—Ä–æ–≥–Ω–æ–∑ —á–µ—Ä–µ–∑ 30 –¥–Ω–µ–π: ~{forecast['forecast_demand_30d']:.0f} –∑–∞–∫–∞–∑–æ–≤/–¥–µ–Ω—å (+{((forecast['forecast_demand_30d'] / forecast['current_demand_estimate']) - 1) * 100:.0f}%)")
            print(f"  –ü–æ–∑–∏—Ü–∏—è –Ω–∞ —Ä—ã–Ω–∫–µ: {forecast['market_position']}")
            print(f"  –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {forecast['recommendation'].upper()}")

        elif args.action == "report":
            if not args.client_id:
                raise ValueError("--client-id –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á—ë—Ç–∞")
            report = await facade.generate_client_report(args.client_id, "month")
            print(f"\n–û—Ç—á—ë—Ç –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞ {args.client_id} —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω")
            print(f"–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ ({len(report['personalized_recommendations'])}):")
            for i, rec in enumerate(report['personalized_recommendations'], 1):
                print(
                    f"  {i}. {rec.get('reason', '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è')} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {rec.get('relevance_score', 0):.0%})")

    asyncio.run(run())


if __name__ == "__main__":
    market_analytics_cli()