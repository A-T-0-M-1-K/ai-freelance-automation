# AI_FREELANCE_AUTOMATION/core/learning/pattern_extractor.py

"""
Pattern Extractor Module
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Extracts behavioral, linguistic, and performance patterns from:
- Completed jobs
- Client conversations
- Feedback & reviews
- Time-to-completion metrics
- Error logs

Used by ContinuousLearningSystem to improve future decisions,
communication tone, bidding strategy, and quality control.

Key Features:
- ML-based pattern detection (clustering, NER, sentiment trends)
- Context-aware pattern tagging
- Anomaly-resistant extraction
- GDPR-compliant data handling
- Hot-pluggable extraction strategies
"""

import logging
import json
from typing import Any, Dict, List, Optional, Set, Tuple
from datetime import datetime
from pathlib import Path

from core.config.unified_config_manager import UnifiedConfigManager
from core.security.audit_logger import AuditLogger
from core.monitoring.metrics_collector import MetricsCollector

# Avoid circular imports: dependencies injected at runtime
class PatternExtractor:
    """
    Stateless, thread-safe pattern extraction engine.
    Designed for use in autonomous learning loops.
    """

    def __init__(
        self,
        config: UnifiedConfigManager,
        audit_logger: Optional[AuditLogger] = None,
        metrics_collector: Optional[MetricsCollector] = None,
    ):
        self.config = config
        self.logger = logging.getLogger("PatternExtractor")
        self.audit_logger = audit_logger or AuditLogger()
        self.metrics = metrics_collector or MetricsCollector()

        # Load extraction rules from config
        self.extraction_rules = self._load_extraction_rules()
        self.enabled_patterns: Set[str] = set(
            self.config.get("learning.patterns.enabled", default=["linguistic", "temporal", "behavioral"])
        )

        self.logger.info("âœ… PatternExtractor initialized with rules: %s", list(self.extraction_rules.keys()))

    def _load_extraction_rules(self) -> Dict[str, Any]:
        """Load pattern extraction rules from config or fallback to defaults."""
        try:
            rules_path = self.config.get("learning.patterns.rules_file")
            if rules_path and Path(rules_path).exists():
                with open(rules_path, "r", encoding="utf-8") as f:
                    return json.load(f)
        except Exception as e:
            self.logger.warning("âš ï¸ Failed to load custom pattern rules: %s. Using defaults.", e)

        # Default rules
        return {
            "linguistic": {
                "keywords": ["urgent", "asap", "quality", "revision", "confidential"],
                "entities": ["PERSON", "ORG", "MONEY", "DATE"],
                "sentiment_shifts": True
            },
            "temporal": {
                "response_time_threshold_sec": 3600,
                "delivery_early_bonus_hours": 12,
                "deadline_pressure_window_hours": 24
            },
            "behavioral": {
                "client_engagement_signals": ["asks_questions", "requests_changes", "gives_positive_feedback"],
                "risk_indicators": ["vague_requirements", "frequent_scope_changes", "late_payments"]
            }
        }

    def extract_patterns_from_job(self, job_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract patterns from a completed job record.
        Input: job_data â€” dict with keys: job_id, client_id, messages, deliverables, feedback, timeline, etc.
        Output: structured pattern dictionary ready for KnowledgeBase ingestion.
        """
        job_id = job_data.get("job_id", "unknown")
        self.logger.debug("ğŸ” Extracting patterns for job: %s", job_id)

        patterns: Dict[str, Any] = {
            "job_id": job_id,
            "client_id": job_data.get("client_id"),
            "extracted_at": datetime.utcnow().isoformat(),
            "patterns": {}
        }

        try:
            if "linguistic" in self.enabled_patterns:
                patterns["patterns"]["linguistic"] = self._extract_linguistic_patterns(job_data)

            if "temporal" in self.enabled_patterns:
                patterns["patterns"]["temporal"] = self._extract_temporal_patterns(job_data)

            if "behavioral" in self.enabled_patterns:
                patterns["patterns"]["behavioral"] = self._extract_behavioral_patterns(job_data)

            self.metrics.increment("pattern_extraction.success")
            self.audit_logger.log("PATTERN_EXTRACTION", f"Patterns extracted for job {job_id}", level="INFO")

        except Exception as e:
            self.logger.error("ğŸ’¥ Pattern extraction failed for job %s: %s", job_id, e, exc_info=True)
            self.metrics.increment("pattern_extraction.failure")
            self.audit_logger.log("PATTERN_EXTRACTION_ERROR", str(e), level="ERROR")
            # Return minimal safe structure to avoid breaking downstream
            patterns["error"] = str(e)
            patterns["patterns"] = {}

        return patterns

    def _extract_linguistic_patterns(self, job_data: Dict[str, Any]) -> Dict[str, Any]:
        messages = job_data.get("messages", [])
        full_text = "\n".join([msg.get("text", "") for msg in messages if isinstance(msg, dict)])

        keywords_found = [
            kw for kw in self.extraction_rules["linguistic"]["keywords"]
            if kw.lower() in full_text.lower()
        ]

        # Placeholder for future NER integration (e.g., spaCy, transformers)
        entities = self._mock_ner_extraction(full_text)

        sentiment_trend = self._analyze_sentiment_trend(messages)

        return {
            "keywords": keywords_found,
            "entities": entities,
            "sentiment_trend": sentiment_trend,
            "text_length_chars": len(full_text),
            "message_count": len(messages)
        }

    def _mock_ner_extraction(self, text: str) -> List[str]:
        """Mock NER â€” replace with real model in production."""
        # In real system: call ai_services.named_entity_recognition(text)
        return []  # Placeholder

    def _analyze_sentiment_trend(self, messages: List[Dict]) -> str:
        """Simple trend: improving, declining, stable."""
        if len(messages) < 2:
            return "insufficient_data"
        # In real system: use sentiment_analyzer on each message
        # For now: assume stable
        return "stable"

    def _extract_temporal_patterns(self, job_data: Dict[str, Any]) -> Dict[str, Any]:
        timeline = job_data.get("timeline", {})
        created = timeline.get("created_at")
        accepted = timeline.get("accepted_at")
        delivered = timeline.get("delivered_at")
        deadline = timeline.get("deadline")

        if not all([created, delivered]):
            return {"error": "missing_timeline_data"}

        try:
            created_dt = datetime.fromisoformat(created.replace("Z", "+00:00"))
            delivered_dt = datetime.fromisoformat(delivered.replace("Z", "+00:00"))
            deadline_dt = datetime.fromisoformat(deadline.replace("Z", "+00:00")) if deadline else None

            duration_sec = (delivered_dt - created_dt).total_seconds()
            early_delivery = deadline_dt and delivered_dt < deadline_dt
            early_hours = (deadline_dt - delivered_dt).total_seconds() / 3600 if early_delivery else 0

            rules = self.extraction_rules["temporal"]
            is_early_bonus = early_hours >= rules["delivery_early_bonus_hours"]
            under_deadline_pressure = (
                deadline_dt and (deadline_dt - datetime.utcnow()).total_seconds() / 3600 <= rules["deadline_pressure_window_hours"]
            )

            return {
                "total_duration_sec": duration_sec,
                "early_delivery_hours": round(early_hours, 2),
                "is_early_bonus_eligible": is_early_bonus,
                "under_deadline_pressure": under_deadline_pressure,
                "deadline_met": not deadline_dt or delivered_dt <= deadline_dt
            }

        except Exception as e:
            self.logger.warning("âš ï¸ Temporal parsing error: %s", e)
            return {"error": "parsing_failed"}

    def _extract_behavioral_patterns(self, job_data: Dict[str, Any]) -> Dict[str, Any]:
        feedback = job_data.get("feedback", {})
        messages = job_data.get("messages", [])
        revisions = job_data.get("revisions", [])

        engagement_signals = []
        risk_indicators = []

        # Simple heuristics
        if any("question" in msg.get("text", "").lower() for msg in messages):
            engagement_signals.append("asks_questions")

        if len(revisions) > 1:
            engagement_signals.append("requests_changes")
            risk_indicators.append("frequent_scope_changes")

        if feedback.get("rating", 0) >= 4:
            engagement_signals.append("gives_positive_feedback")

        if not job_data.get("requirements"):
            risk_indicators.append("vague_requirements")

        # Payment data would come from finances/ â€” omitted here for separation of concerns

        return {
            "engagement_signals": engagement_signals,
            "risk_indicators": risk_indicators,
            "revision_count": len(revisions),
            "has_feedback": bool(feedback)
        }

    def get_supported_pattern_types(self) -> List[str]:
        return list(self.extraction_rules.keys())
