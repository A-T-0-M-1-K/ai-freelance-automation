# AI_FREELANCE_AUTOMATION/core/ai_management/intelligent_model_manager.py

"""
Intelligent Model Manager ‚Äî —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è AI-–º–æ–¥–µ–ª—è–º–∏.
–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç:
- –î–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –∑–∞–≥—Ä—É–∑–∫—É/–≤—ã–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–µ–π
- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- –ü–æ–¥–¥–µ—Ä–∂–∫—É –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ (–ª–æ–∫–∞–ª—å–Ω—ã–µ + API)
- –°–∞–º–æ–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∏ —Å–±–æ—è—Ö
- –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –ø–ª–∞–≥–∏–Ω–∞–º–∏ AI
"""

import asyncio
import logging
import time
from typing import Dict, Any, Optional, Union, List
from pathlib import Path

from core.config.unified_config_manager import UnifiedConfigManager
from core.performance.intelligent_cache_system import IntelligentCacheSystem
from core.performance.memory_optimizer import MemoryOptimizer
from core.ai_management.model_registry import ModelRegistry
from core.security.advanced_crypto_system import AdvancedCryptoSystem
from core.monitoring.metrics_collector import MetricsCollector

# –¢–∏–ø—ã –º–æ–¥–µ–ª–µ–π
ModelType = str  # –Ω–∞–ø—Ä–∏–º–µ—Ä: "whisper-medium", "gpt-4-turbo", "nllb-200"
ModelInstance = Any  # –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–æ–¥–µ–ª–∏


class IntelligentModelManager:
    """
    –£–ø—Ä–∞–≤–ª—è–µ—Ç –≤—Å–µ–º–∏ AI-–º–æ–¥–µ–ª—è–º–∏ –≤ —Å–∏—Å—Ç–µ–º–µ.
    –†–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ —Ñ–∞–±—Ä–∏–∫–∞ + –ø—É–ª + –º–æ–Ω–∏—Ç–æ—Ä.
    """

    def __init__(
        self,
        config: UnifiedConfigManager,
        crypto: AdvancedCryptoSystem,
        cache: Optional[IntelligentCacheSystem] = None,
        memory_optimizer: Optional[MemoryOptimizer] = None,
        metrics_collector: Optional[MetricsCollector] = None
    ):
        self.config = config
        self.crypto = crypto
        self.cache = cache or IntelligentCacheSystem(config)
        self.memory_optimizer = memory_optimizer or MemoryOptimizer(config)
        self.metrics = metrics_collector or MetricsCollector()
        self.logger = logging.getLogger("IntelligentModelManager")

        # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        self._loaded_models: Dict[ModelType, ModelInstance] = {}
        self._model_load_times: Dict[ModelType, float] = {}
        self._model_usage_count: Dict[ModelType, int] = {}
        self._model_last_used: Dict[ModelType, float] = {}

        # –†–µ–≥–∏—Å—Ç—Ä –º–æ–¥–µ–ª–µ–π (—Å–æ–¥–µ—Ä–∂–∏—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: –∏—Å—Ç–æ—á–Ω–∏–∫, —Ç–∏–ø, —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è)
        self.registry = ModelRegistry(config)

        # –§–ª–∞–≥–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        self._initialized = False
        self._shutdown = False

        self.logger.info("Intialized IntelligentModelManager")

    async def initialize(self) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞: –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π."""
        if self._initialized:
            return
        self.logger.info("üîÑ Initializing AI models...")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏, –ø–æ–º–µ—á–µ–Ω–Ω—ã–µ –∫–∞–∫ 'preload'
        preload_models = self.config.get("ai.preload_models", [])
        for model_name in preload_models:
            try:
                await self.get_model(model_name)
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Failed to preload model '{model_name}': {e}")

        self._initialized = True
        self.logger.info("‚úÖ AI Model Manager initialized")

    async def get_model(self, model_name: ModelType) -> ModelInstance:
        """
        –ü–æ–ª—É—á–∏—Ç—å —ç–∫–∑–µ–º–ø–ª—è—Ä –º–æ–¥–µ–ª–∏ –ø–æ –∏–º–µ–Ω–∏.
        –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ ‚Äî –∑–∞–≥—Ä—É–∂–∞–µ—Ç –µ—ë —Å —É—á—ë—Ç–æ–º —Ä–µ—Å—É—Ä—Å–æ–≤ –∏ –∫—ç—à–∞.
        """
        if self._shutdown:
            raise RuntimeError("Model manager is shutting down")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cached = self.cache.get(f"model:{model_name}")
        if cached and not self._is_model_stale(model_name):
            self._update_usage_stats(model_name)
            self.metrics.increment("ai.model.cache_hit", tags={"model": model_name})
            return cached

        # –ï—Å–ª–∏ –Ω–µ –≤ –∫—ç—à–µ ‚Äî –∑–∞–≥—Ä—É–∂–∞–µ–º
        if model_name not in self._loaded_models:
            await self._load_model(model_name)

        instance = self._loaded_models[model_name]
        self.cache.set(f"model:{model_name}", instance, ttl=3600)  # –∫—ç—à –Ω–∞ 1 —á–∞—Å
        self._update_usage_stats(model_name)
        self.metrics.increment("ai.model.load", tags={"model": model_name})
        return instance

    async def _load_model(self, model_name: ModelType) -> None:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —Å —É—á—ë—Ç–æ–º —Ç–∏–ø–∞ (–ª–æ–∫–∞–ª—å–Ω–∞—è / API / –ø–ª–∞–≥–∏–Ω)."""
        self.logger.info(f"üì• Loading model: {model_name}")

        model_info = self.registry.get_model_info(model_name)
        if not model_info:
            raise ValueError(f"Unknown model: {model_name}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Ä–µ—Å—É—Ä—Å–æ–≤
        required_memory = model_info.get("memory_mb", 1024)
        if not self.memory_optimizer.can_allocate(required_memory):
            # –í—ã–≥—Ä—É–∂–∞–µ–º –Ω–∞–∏–º–µ–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º—É—é –º–æ–¥–µ–ª—å
            await self._evict_least_used_model()

        try:
            start_time = time.time()
            provider = model_info.get("provider", "local")
            model_path = model_info.get("path")
            api_key = None

            if provider == "openai":
                from ai_plugins.openai_plugin import OpenAIModelAdapter
                api_key = self.crypto.decrypt_secret("OPENAI_API_KEY")
                instance = OpenAIModelAdapter(model_name, api_key=api_key)
            elif provider == "anthropic":
                from ai_plugins.claude_plugin import ClaudeModelAdapter
                api_key = self.crypto.decrypt_secret("ANTHROPIC_API_KEY")
                instance = ClaudeModelAdapter(model_name, api_key=api_key)
            elif provider == "local":
                if not model_path:
                    raise ValueError(f"Local model '{model_name}' requires 'path' in registry")
                model_path = Path(model_path)
                if not model_path.exists():
                    raise FileNotFoundError(f"Model path not found: {model_path}")
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º sandboxed loader
                instance = await self._load_local_model_safely(model_path, model_name)
            else:
                # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ø–ª–∞–≥–∏–Ω–æ–≤
                plugin_class = self._load_plugin_model(provider, model_name)
                instance = plugin_class(model_info)

            self._loaded_models[model_name] = instance
            self._model_load_times[model_name] = time.time() - start_time
            self.logger.info(f"‚úÖ Model '{model_name}' loaded in {self._model_load_times[model_name]:.2f}s")

        except Exception as e:
            self.logger.error(f"üí• Failed to load model '{model_name}': {e}", exc_info=True)
            self.metrics.increment("ai.model.load_failure", tags={"model": model_name})
            raise

    async def _load_local_model_safely(self, path: Path, model_name: str) -> ModelInstance:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ä–µ–¥–µ."""
        # TODO: –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å subprocess –∏–ª–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–∞—Ü–∏—é
        # –ó–¥–µ—Å—å ‚Äî –±–∞–∑–æ–≤–∞—è –∑–∞—â–∏—Ç–∞ —á–µ—Ä–µ–∑ try/except –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø—É—Ç–µ–π
        if ".." in str(path) or not str(path).startswith("ai/models/"):
            raise ValueError("Invalid model path (security violation)")

        if "whisper" in model_name:
            from ai_services.transcription_service import WhisperModelLoader
            return WhisperModelLoader.load(str(path))
        elif "gpt" in model_name or "llama" in model_name:
            from ai_services.copywriting_service import TransformerModelLoader
            return TransformerModelLoader.load(str(path))
        else:
            raise NotImplementedError(f"Unsupported local model type: {model_name}")

    def _load_plugin_model(self, provider: str, model_name: str) -> type:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–ª–∞—Å—Å –º–æ–¥–µ–ª–∏ –∏–∑ –ø–ª–∞–≥–∏–Ω–∞."""
        try:
            plugin_module = f"plugins.ai_plugins.{provider}_plugin"
            plugin = __import__(plugin_module, fromlist=["get_model_class"])
            return plugin.get_model_class(model_name)
        except ImportError as e:
            raise ImportError(f"Plugin for provider '{provider}' not found: {e}")

    def _is_model_stale(self, model_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —É—Å—Ç–∞—Ä–µ–ª–∞ –ª–∏ –º–æ–¥–µ–ª—å –≤ –∫—ç—à–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ—Å–ª–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥–∞)."""
        last_used = self._model_last_used.get(model_name, 0)
        ttl = self.config.get("ai.model_cache_ttl_seconds", 3600)
        return (time.time() - last_used) > ttl

    def _update_usage_stats(self, model_name: str) -> None:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏."""
        self._model_usage_count[model_name] = self._model_usage_count.get(model_name, 0) + 1
        self._model_last_used[model_name] = time.time()

    async def _evict_least_used_model(self) -> None:
        """–í—ã–≥—Ä—É–∂–∞–µ—Ç –Ω–∞–∏–º–µ–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏."""
        if not self._loaded_models:
            return

        # –ù–∞—Ö–æ–¥–∏–º –º–æ–¥–µ–ª—å —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º usage –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –≤—Ä–µ–º–µ–Ω–µ–º –±–µ–∑–¥–µ–π—Å—Ç–≤–∏—è
        candidate = min(
            self._loaded_models.keys(),
            key=lambda m: (
                self._model_usage_count.get(m, 0),
                -self._model_last_used.get(m, 0)  # —á–µ–º —Å—Ç–∞—Ä—à–µ ‚Äî —Ç–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–µ–µ –≤—ã–≥—Ä—É–∑–∫–∞
            )
        )

        self.logger.info(f"üì§ Evicting least-used model: {candidate}")
        await self.unload_model(candidate)

    async def unload_model(self, model_name: ModelType) -> None:
        """–í—ã–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ –ø–∞–º—è—Ç–∏ –∏ –æ—á–∏—â–∞–µ—Ç –∫—ç—à."""
        if model_name not in self._loaded_models:
            return

        instance = self._loaded_models.pop(model_name)
        self.cache.delete(f"model:{model_name}")

        # –í—ã–∑—ã–≤–∞–µ–º cleanup, –µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        if hasattr(instance, "cleanup"):
            try:
                await instance.cleanup()
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Error during model cleanup: {e}")

        self.logger.info(f"üóëÔ∏è Model '{model_name}' unloaded")
        self.metrics.increment("ai.model.unload", tags={"model": model_name})

    async def shutdown(self) -> None:
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã: –≤—ã–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π."""
        self._shutdown = True
        self.logger.info("üõë Shutting down IntelligentModelManager...")

        tasks = [self.unload_model(name) for name in list(self._loaded_models.keys())]
        await asyncio.gather(*tasks, return_exceptions=True)

        self.logger.info("‚úÖ Model manager shut down complete")

    def get_model_performance_report(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç—á—ë—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π."""
        return {
            "loaded_models": list(self._loaded_models.keys()),
            "load_times_sec": self._model_load_times,
            "usage_counts": self._model_usage_count,
            "last_used_timestamps": self._model_last_used,
            "memory_usage_mb": self.memory_optimizer.get_current_usage(),
        }