# AI_FREELANCE_AUTOMATION/core/ai_management/model_optimizer.py

"""
Model Optimizer ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å AI-–º–æ–¥–µ–ª–µ–π
–≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç—Ä–∏–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è, –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ä–µ—Å—É—Ä—Å–æ–≤.

–§—É–Ω–∫—Ü–∏–∏:
- –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ (latency, accuracy, memory)
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–º–µ–Ω–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –±–æ–ª–µ–µ –ª–µ–≥–∫—É—é/—Ç–æ—á–Ω—É—é –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
- –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ, pruning, distillation (–µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)
- –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≤ UnifiedConfigManager
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å continuous_learning –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø–æ–¥ –¥–æ–º–µ–Ω

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- –ù–µ –∑–∞–≤–∏—Å–∏—Ç –Ω–∞–ø—Ä—è–º—É—é –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤ (PyTorch/TensorFlow –∞–±—Å—Ç—Ä–∞–≥–∏—Ä–æ–≤–∞–Ω—ã)
- –†–∞–±–æ—Ç–∞–µ—Ç —á–µ—Ä–µ–∑ service locator –∏–ª–∏ DI
- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≥–æ—Ä—è—á—É—é –∑–∞–º–µ–Ω—É –º–æ–¥–µ–ª–µ–π –±–µ–∑ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ —Å–∏—Å—Ç–µ–º—ã
"""

import logging
import time
from typing import Dict, Any, Optional, Callable
from pathlib import Path

from core.config.unified_config_manager import UnifiedConfigManager
from core.monitoring.intelligent_monitoring_system import IntelligentMonitoringSystem
from core.ai_management.model_registry import ModelRegistry
from core.learning.continuous_learning_system import ContinuousLearningSystem


class ModelOptimizer:
    """
    –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä AI-–º–æ–¥–µ–ª–µ–π. –†–∞–±–æ—Ç–∞–µ—Ç –≤ —Ñ–æ–Ω–µ –∏ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç,
    –º–æ–∂–Ω–æ –ª–∏ —É–ª—É—á—à–∏—Ç—å –º–æ–¥–µ–ª—å –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏, —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏–ª–∏ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—é –ø–∞–º—è—Ç–∏.
    """

    def __init__(
        self,
        config_manager: UnifiedConfigManager,
        monitoring_system: IntelligentMonitoringSystem,
        model_registry: ModelRegistry,
        learning_system: Optional[ContinuousLearningSystem] = None,
        optimization_interval_seconds: int = 3600  # —Ä–∞–∑ –≤ —á–∞—Å
    ):
        self.config = config_manager
        self.monitoring = monitoring_system
        self.registry = model_registry
        self.learning = learning_system
        self.interval = optimization_interval_seconds
        self.logger = logging.getLogger("ModelOptimizer")
        self._running = False
        self._last_optimization: Dict[str, float] = {}

        self.optimization_strategies = {
            "quantize": self._apply_quantization,
            "prune": self._apply_pruning,
            "distill": self._apply_distillation,
            "switch_to_lighter": self._switch_to_lighter_model,
            "fine_tune": self._trigger_fine_tuning,
        }

        self.logger.info("Intialized ModelOptimizer with %d strategies", len(self.optimization_strategies))

    async def start_optimization_loop(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ñ–æ–Ω–æ–≤—ã–π —Ü–∏–∫–ª –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏."""
        if self._running:
            self.logger.warning("Optimization loop already running")
            return
        self._running = True
        self.logger.info("‚ñ∂Ô∏è Starting model optimization loop (interval: %ds)", self.interval)

        while self._running:
            try:
                await self._run_optimization_cycle()
            except Exception as e:
                self.logger.error("‚ùå Error in optimization cycle: %s", e, exc_info=True)
                # –ù–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ü–∏–∫–ª ‚Äî —Å–∏—Å—Ç–µ–º–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –æ—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤–æ–π
            await asyncio.sleep(self.interval)

    def stop(self):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Ü–∏–∫–ª –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏."""
        self._running = False
        self.logger.info("‚èπÔ∏è Model optimization loop stopped")

    async def _run_optimization_cycle(self):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–∏–Ω —Ü–∏–∫–ª –∞–Ω–∞–ª–∏–∑–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
        self.logger.debug("üîç Starting optimization cycle...")
        active_models = self.registry.get_active_models()

        for model_id in active_models:
            try:
                await self._optimize_single_model(model_id)
            except Exception as e:
                self.logger.error("üí• Failed to optimize model %s: %s", model_id, e, exc_info=True)

        self.logger.debug("‚úÖ Optimization cycle completed")

    async def _optimize_single_model(self, model_id: str):
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –æ–¥–Ω—É –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ—ë –º–µ—Ç—Ä–∏–∫."""
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        metrics = await self.monitoring.get_model_metrics(model_id)
        if not metrics:
            self.logger.debug("No metrics for model %s, skipping", model_id)
            return

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø—Ä–æ—à–ª–æ –ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—Ä–µ–º–µ–Ω–∏ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        last_opt = self._last_optimization.get(model_id, 0)
        if time.time() - last_opt < self.interval:
            return

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        need_optimize = self._should_optimize(metrics)
        if not need_optimize:
            return

        self.logger.info("‚ö° Optimization needed for model %s: %s", model_id, need_optimize)

        # –í—ã–±–∏—Ä–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
        strategy_name = self._select_optimization_strategy(metrics, need_optimize)
        if strategy_name not in self.optimization_strategies:
            self.logger.warning("Unknown strategy: %s for model %s", strategy_name, model_id)
            return

        strategy = self.optimization_strategies[strategy_name]
        try:
            result = await strategy(model_id, metrics)
            if result:
                self._last_optimization[model_id] = time.time()
                self.logger.info("‚úÖ Successfully applied '%s' to model %s", strategy_name, model_id)
                # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–µ–µ—Å—Ç—Ä –∏ –∫–æ–Ω—Ñ–∏–≥
                self.registry.mark_model_as_optimized(model_id, strategy_name, result)
                await self.config.update_model_config(model_id, result.get("new_config", {}))
        except Exception as e:
            self.logger.error("‚ö†Ô∏è Strategy '%s' failed for model %s: %s", strategy_name, model_id, e)

    def _should_optimize(self, metrics: Dict[str, Any]) -> Optional[str]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–∏—á–∏–Ω—É –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ None, –µ—Å–ª–∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.
        –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã: 'high_latency', 'low_accuracy', 'high_memory', 'low_throughput'
        """
        latency = metrics.get("avg_inference_time_sec", 0)
        accuracy = metrics.get("accuracy", 1.0)
        memory = metrics.get("peak_memory_mb", 0)
        throughput = metrics.get("requests_per_minute", float('inf'))

        threshold = self.config.get("ai.optimization.thresholds", {})

        if latency > threshold.get("max_latency_sec", 5.0):
            return "high_latency"
        if accuracy < threshold.get("min_accuracy", 0.85):
            return "low_accuracy"
        if memory > threshold.get("max_memory_mb", 2048):
            return "high_memory"
        if throughput < threshold.get("min_throughput_rpm", 10):
            return "low_throughput"

        return None

    def _select_optimization_strategy(
        self, metrics: Dict[str, Any], reason: str
    ) -> str:
        """–í—ã–±–∏—Ä–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏—á–∏–Ω—ã –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."""
        model_type = metrics.get("model_type", "unknown")
        is_local = metrics.get("is_local", True)

        if reason == "high_latency" and is_local:
            return "quantize"
        if reason == "high_memory":
            return "prune"
        if reason == "low_accuracy" and self.learning:
            return "fine_tune"
        if reason in ("high_latency", "high_memory") and is_local:
            return "switch_to_lighter"
        # fallback
        return "quantize"

    async def _apply_quantization(self, model_id: str, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ (INT8/FP16) –∫ –º–æ–¥–µ–ª–∏."""
        self.logger.info("üîß Applying quantization to %s", model_id)
        # –ó–¥–µ—Å—å –±—É–¥–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å ONNX Runtime, TensorRT, –∏–ª–∏ HuggingFace Optimum
        # –î–ª—è –∫–∞—Ä–∫–∞—Å–∞ ‚Äî —Å–∏–º—É–ª—è—Ü–∏—è
        return {
            "strategy": "quantize",
            "new_config": {
                "precision": "int8",
                "expected_speedup": 1.8,
                "expected_memory_reduction": 0.6
            },
            "status": "applied"
        }

    async def _apply_pruning(self, model_id: str, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç pruning (—É–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–æ–≤/—Å–ª–æ—ë–≤)."""
        self.logger.info("‚úÇÔ∏è Applying pruning to %s", model_id)
        return {
            "strategy": "prune",
            "new_config": {
                "sparsity": 0.3,
                "expected_memory_reduction": 0.4
            },
            "status": "applied"
        }

    async def _apply_distillation(self, model_id: str, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç knowledge distillation –æ—Ç –±–æ–ª—å—à–æ–π –º–æ–¥–µ–ª–∏ –∫ –º–∞–ª–æ–π."""
        self.logger.info("üéì Starting distillation for %s", model_id)
        raise NotImplementedError("Distillation requires teacher model ‚Äî not implemented in base version")

    async def _switch_to_lighter_model(self, model_id: str, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """–ü–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç—Å—è –Ω–∞ –±–æ–ª–µ–µ –ª—ë–≥–∫—É—é –≤–µ—Ä—Å–∏—é –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, small –≤–º–µ—Å—Ç–æ medium)."""
        current_name = metrics.get("model_name", "")
        if "medium" in current_name:
            new_name = current_name.replace("medium", "small")
        elif "large" in current_name:
            new_name = current_name.replace("large", "medium")
        else:
            new_name = current_name + "_lite"

        self.logger.info("üîÑ Switching %s ‚Üí %s", current_name, new_name)
        return {
            "strategy": "switch_to_lighter",
            "new_config": {
                "model_name": new_name,
                "auto_loaded": True
            },
            "status": "switched"
        }

    async def _trigger_fine_tuning(self, model_id: str, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """–ò–Ω–∏—Ü–∏–∏—Ä—É–µ—Ç –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∏–¥–±—ç–∫–∞ –∏–∑ ContinuousLearningSystem."""
        if not self.learning:
            raise RuntimeError("Fine-tuning requires ContinuousLearningSystem")

        self.logger.info("üß† Triggering fine-tuning for %s based on feedback", model_id)
        job_samples = await self.learning.get_recent_feedback_samples(model_id, n=100)
        task = await self.learning.create_finetune_task(model_id, job_samples)

        return {
            "strategy": "fine_tune",
            "new_config": {
                "finetune_task_id": task["id"],
                "status": "queued"
            },
            "status": "fine_tuning_started"
        }