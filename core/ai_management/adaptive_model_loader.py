# –§–∞–π–ª: core/ai_management/adaptive_model_loader.py
"""
–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π –ò–ò —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –≤—ã–±–æ—Ä–æ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
–ø–æ–¥ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ (–ü–ö/–Ω–æ—É—Ç–±—É–∫ –±–µ–∑ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—ã)
"""
import os
import psutil
import torch
import logging
from typing import Dict, Any, Optional, Tuple
from pathlib import Path
from enum import Enum
from transformers import AutoModel, AutoTokenizer, pipeline

logger = logging.getLogger(__name__)


class DeviceCapability(Enum):
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
    HIGH_END_GPU = "high_end_gpu"  # GPU —Å 8+ –ì–ë VRAM (RTX 3070+)
    MID_RANGE_GPU = "mid_range_gpu"  # GPU —Å 4-8 –ì–ë VRAM (GTX 1660 / RTX 3050)
    INTEGRATED_GPU = "integrated_gpu"  # –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥—Ä–∞—Ñ–∏–∫–∞ (Intel Iris / AMD Vega)
    CPU_ONLY = "cpu_only"  # –¢–æ–ª—å–∫–æ CPU (–Ω–æ—É—Ç–±—É–∫–∏ –±–µ–∑ GPU)


class ModelVariant(Enum):
    """–í–∞—Ä–∏–∞–Ω—Ç—ã –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤"""
    FULL = "full"  # –ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è)
    QUANTIZED_INT8 = "int8"  # –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–∞—è 8-–±–∏—Ç
    QUANTIZED_INT4 = "int4"  # –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–∞—è 4-–±–∏—Ç (–¥–ª—è —Å–ª–∞–±—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤)
    DISTILLED = "distilled"  # –î–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ª–µ–≥–∫–∞—è –≤–µ—Ä—Å–∏—è


@dataclass
class DeviceProfile:
    """–ü—Ä–æ—Ñ–∏–ª—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫"""
    total_ram_gb: float
    available_ram_gb: float
    has_gpu: bool
    gpu_name: Optional[str]
    gpu_vram_gb: Optional[float]
    cpu_cores: int
    capability: DeviceCapability
    recommended_variant: ModelVariant


class AdaptiveModelLoader:
    """
    –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π
    –ø–æ–¥ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    """

    def __init__(self, base_model_dir: str = "ai/models"):
        self.base_model_dir = Path(base_model_dir)
        self.device_profile = self._detect_device_capabilities()
        self.loaded_models: Dict[str, Any] = {}
        self.model_variants: Dict[str, Dict[str, str]] = self._define_model_variants()

        logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω –ø—Ä–æ—Ñ–∏–ª—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞: {self.device_profile.capability.value}")
        logger.info(f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –º–æ–¥–µ–ª–µ–π: {self.device_profile.recommended_variant.value}")
        logger.info(
            f"–î–æ—Å—Ç—É–ø–Ω–æ –û–ó–£: {self.device_profile.available_ram_gb:.1f} –ì–ë –∏–∑ {self.device_profile.total_ram_gb:.1f} –ì–ë")
        if self.device_profile.has_gpu:
            logger.info(f"GPU: {self.device_profile.gpu_name} —Å {self.device_profile.gpu_vram_gb:.1f} –ì–ë VRAM")

    def _detect_device_capabilities(self) -> DeviceProfile:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –û–ó–£
        total_ram = psutil.virtual_memory().total / (1024 ** 3)
        available_ram = psutil.virtual_memory().available / (1024 ** 3)
        cpu_cores = psutil.cpu_count(logical=True)

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ GPU
        has_gpu = torch.cuda.is_available()
        gpu_name = None
        gpu_vram = None

        if has_gpu:
            gpu_name = torch.cuda.get_device_name(0)
            gpu_vram = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        if has_gpu and gpu_vram >= 8.0:
            capability = DeviceCapability.HIGH_END_GPU
            recommended_variant = ModelVariant.FULL
        elif has_gpu and gpu_vram >= 4.0:
            capability = DeviceCapability.MID_RANGE_GPU
            recommended_variant = ModelVariant.QUANTIZED_INT8
        elif has_gpu:
            capability = DeviceCapability.INTEGRATED_GPU
            recommended_variant = ModelVariant.QUANTIZED_INT4
        elif total_ram >= 16.0:
            capability = DeviceCapability.CPU_ONLY
            recommended_variant = ModelVariant.DISTILLED
        else:
            capability = DeviceCapability.CPU_ONLY
            recommended_variant = ModelVariant.QUANTIZED_INT4

        return DeviceProfile(
            total_ram_gb=total_ram,
            available_ram_gb=available_ram,
            has_gpu=has_gpu,
            gpu_name=gpu_name,
            gpu_vram_gb=gpu_vram,
            cpu_cores=cpu_cores,
            capability=capability,
            recommended_variant=recommended_variant
        )

    def _define_model_variants(self) -> Dict[str, Dict[str, str]]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—É—Ç–µ–π –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –≤–∞—Ä–∏–∞–Ω—Ç–∞–º –º–æ–¥–µ–ª–µ–π"""
        return {
            "embedding": {
                "full": "bert-base-multilingual",
                "distilled": "distilbert-base-multilingual-cased",
                "quantized_int8": "bert-base-multilingual-int8",
                "quantized_int4": "bert-base-multilingual-int4"
            },
            "textgen": {
                "full": "gpt2-medium",
                "distilled": "gpt2",
                "quantized_int8": "gpt2-medium-int8",
                "quantized_int4": "gpt2-medium-int4"
            },
            "translation": {
                "full": "nllb-200",
                "distilled": "nllb-200-distilled-600M",
                "quantized_int8": "nllb-200-int8",
                "quantized_int4": "nllb-200-int4"
            },
            "whisper": {
                "full": "whisper-medium",
                "distilled": "whisper-small",
                "quantized_int8": "whisper-medium-int8",
                "quantized_int4": "whisper-small-int4"
            }
        }

    def get_optimal_variant(self, model_type: str) -> Tuple[str, ModelVariant]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        """
        variants = self.model_variants.get(model_type, {})
        recommended = self.device_profile.recommended_variant.value

        # –ü–æ–∏—Å–∫ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞
        if recommended in variants:
            return variants[recommended], ModelVariant(recommended)

        # –†–µ–∑–µ—Ä–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –≤ –ø–æ—Ä—è–¥–∫–µ —É–±—ã–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        fallback_order = ["distilled", "quantized_int8", "quantized_int4", "full"]
        for variant in fallback_order:
            if variant in variants:
                return variants[variant], ModelVariant(variant)

        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–ª–Ω—É—é –º–æ–¥–µ–ª—å –∫–∞–∫ —Ä–µ–∑–µ—Ä–≤
        return variants.get("full", variants[list(variants.keys())[0]]), ModelVariant.FULL

    async def load_model(self, model_type: str, force_variant: Optional[ModelVariant] = None) -> Any:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π –ø–æ–¥ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        """
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞
        if force_variant:
            variant_name = force_variant.value
            model_path = self.model_variants[model_type].get(variant_name)
            if not model_path:
                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ñ–æ–ª–±—ç–∫ –Ω–∞ –¥–æ—Å—Ç—É–ø–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
                model_path, detected_variant = self.get_optimal_variant(model_type)
                logger.warning(
                    f"–í–∞—Ä–∏–∞–Ω—Ç {variant_name} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è {model_type}, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {detected_variant.value}")
        else:
            model_path, variant = self.get_optimal_variant(model_type)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∏—Å–∫–µ
        full_path = self.base_model_dir / model_path
        if not full_path.exists():
            logger.info(f"–ú–æ–¥–µ–ª—å {model_path} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞...")
            await self._download_model(model_type, variant)

        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        logger.info(
            f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_type} ({variant.value}) –¥–ª—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ {self.device_profile.capability.value}")

        try:
            if model_type == "embedding":
                return self._load_embedding_model(full_path, variant)
            elif model_type == "textgen":
                return self._load_textgen_model(full_path, variant)
            elif model_type == "translation":
                return self._load_translation_model(full_path, variant)
            elif model_type == "whisper":
                return self._load_whisper_model(full_path, variant)
            else:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {model_type}")

        except RuntimeError as e:
            if "out of memory" in str(e).lower() or "cuda out of memory" in str(e).lower():
                logger.warning(
                    f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ {model_type} ({variant.value}), –ø—Ä–æ–±—É–µ–º –±–æ–ª–µ–µ –ª–µ–≥–∫–∏–π –≤–∞—Ä–∏–∞–Ω—Ç...")
                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–µ—Ö–æ–¥ –Ω–∞ –±–æ–ª–µ–µ –ª–µ–≥–∫–∏–π –≤–∞—Ä–∏–∞–Ω—Ç
                lighter_variants = {
                    ModelVariant.FULL: ModelVariant.QUANTIZED_INT8,
                    ModelVariant.QUANTIZED_INT8: ModelVariant.QUANTIZED_INT4,
                    ModelVariant.QUANTIZED_INT4: ModelVariant.DISTILLED,
                    ModelVariant.DISTILLED: ModelVariant.QUANTIZED_INT4  # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–π —Ñ–æ–ª–±—ç–∫
                }
                new_variant = lighter_variants.get(variant, ModelVariant.QUANTIZED_INT4)
                return await self.load_model(model_type, force_variant=new_variant)
            else:
                raise

    def _load_embedding_model(self, path: Path, variant: ModelVariant):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –¥–ª—è —Å–ª–∞–±—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤"""
        device = "cuda" if self.device_profile.has_gpu and self.device_profile.gpu_vram_gb >= 2.0 else "cpu"

        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        load_kwargs = {}
        if variant == ModelVariant.QUANTIZED_INT8:
            load_kwargs["load_in_8bit"] = True
        elif variant == ModelVariant.QUANTIZED_INT4:
            load_kwargs["load_in_4bit"] = True

        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        model = AutoModel.from_pretrained(
            str(path),
            device_map="auto" if device == "cuda" else None,
            **load_kwargs
        )
        tokenizer = AutoTokenizer.from_pretrained(str(path))

        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è CPU
        if device == "cpu" and variant != ModelVariant.QUANTIZED_INT4:
            model = torch.quantization.quantize_dynamic(
                model, {torch.nn.Linear}, dtype=torch.qint8
            )
            logger.info("–ü—Ä–∏–º–µ–Ω–µ–Ω–æ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –¥–ª—è CPU")

        return {"model": model, "tokenizer": tokenizer, "device": device, "variant": variant.value}

    def _load_textgen_model(self, path: Path, variant: ModelVariant):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–ª–∏–Ω—ã –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏"""
        device = "cuda" if self.device_profile.has_gpu and self.device_profile.gpu_vram_gb >= 3.0 else "cpu"

        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        max_length = {
            DeviceCapability.HIGH_END_GPU: 1024,
            DeviceCapability.MID_RANGE_GPU: 512,
            DeviceCapability.INTEGRATED_GPU: 256,
            DeviceCapability.CPU_ONLY: 128
        }.get(self.device_profile.capability, 256)

        return pipeline(
            "text-generation",
            model=str(path),
            device=0 if device == "cuda" else -1,
            max_length=max_length,
            torch_dtype=torch.float16 if device == "cuda" and variant != ModelVariant.QUANTIZED_INT4 else torch.float32
        )

    def _load_whisper_model(self, path: Path, variant: ModelVariant):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Whisper —Å –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π –ø–æ–¥ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
        device = "cuda" if self.device_profile.has_gpu and self.device_profile.gpu_vram_gb >= 2.0 else "cpu"

        # –í—ã–±–æ—Ä —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–∞–º—è—Ç–∏
        model_size = "medium" if self.device_profile.gpu_vram_gb and self.device_profile.gpu_vram_gb >= 4.0 else "small"

        return pipeline(
            "automatic-speech-recognition",
            model=f"openai/whisper-{model_size}",
            device=0 if device == "cuda" else -1,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            chunk_length_s=30,  # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è —Å–ª–∞–±—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤
            stride_length_s=5
        )

    async def _download_model(self, model_type: str, variant: ModelVariant):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º"""
        import huggingface_hub

        model_map = {
            "embedding": {
                "full": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
                "distilled": "sentence-transformers/distiluse-base-multilingual-cased-v1",
                "quantized_int8": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
                # –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ
                "quantized_int4": "sentence-transformers/distiluse-base-multilingual-cased-v1"
            },
            "textgen": {
                "full": "gpt2-medium",
                "distilled": "gpt2",
                "quantized_int8": "gpt2-medium",
                "quantized_int4": "gpt2"
            },
            "translation": {
                "full": "facebook/nllb-200-3.3B",
                "distilled": "facebook/nllb-200-distilled-600M",
                "quantized_int8": "facebook/nllb-200-distilled-600M",
                "quantized_int4": "facebook/nllb-200-distilled-600M"
            },
            "whisper": {
                "full": "openai/whisper-medium",
                "distilled": "openai/whisper-small",
                "quantized_int8": "openai/whisper-small",
                "quantized_int4": "openai/whisper-small"
            }
        }

        model_name = model_map[model_type][variant.value]
        save_path = self.base_model_dir / self.model_variants[model_type][variant.value]

        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name} –≤ {save_path}...")

        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        save_path.parent.mkdir(parents=True, exist_ok=True)

        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º
        huggingface_hub.snapshot_download(
            repo_id=model_name,
            local_dir=str(save_path),
            progress=True
        )

        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –¥–ª—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
        if variant in [ModelVariant.QUANTIZED_INT8, ModelVariant.QUANTIZED_INT4]:
            await self._apply_quantization(save_path, variant)

        logger.info(f"–ú–æ–¥–µ–ª—å {model_type} ({variant.value}) —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ {save_path}")

    async def _apply_quantization(self, model_path: Path, variant: ModelVariant):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer
            from transformers import BitsAndBytesConfig

            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è
            model = AutoModelForCausalLM.from_pretrained(str(model_path))
            tokenizer = AutoTokenizer.from_pretrained(str(model_path))

            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è
            if variant == ModelVariant.QUANTIZED_INT8:
                quantization_config = BitsAndBytesConfig(load_in_8bit=True)
            else:  # QUANTIZED_INT4
                quantization_config = BitsAndBytesConfig(load_in_4bit=True)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            model.save_pretrained(str(model_path), quantization_config=quantization_config)
            tokenizer.save_pretrained(str(model_path))

            logger.info(f"–ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ {variant.value} –ø—Ä–∏–º–µ–Ω–µ–Ω–æ –∫ –º–æ–¥–µ–ª–∏ –≤ {model_path}")

        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {str(e)}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å.")

    def get_performance_recommendations(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        recommendations = []

        if self.device_profile.capability == DeviceCapability.CPU_ONLY:
            recommendations.extend([
                "‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–µ–∂–∏–º —Ç–æ–ª—å–∫–æ CPU ‚Äî –æ–∂–∏–¥–∞–π—Ç–µ –∑–∞–º–µ–¥–ª–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ò–ò –Ω–∞ 5-10x",
                "üí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: –∑–∞–∫—Ä—ã—Ç—å –¥—Ä—É–≥–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –û–ó–£",
                "üí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (int4) –¥–ª—è –≤—Å–µ—Ö –∑–∞–¥–∞—á",
                "üí° –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –ø–æ–¥–∫–ª—é—á–∏—Ç—å –≤–Ω–µ—à–Ω—é—é –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—É —á–µ—Ä–µ–∑ eGPU (–¥–ª—è –Ω–æ—É—Ç–±—É–∫–æ–≤)",
                "‚ö° –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –≤–∫–ª—é—á–∏—Ç—å '—Ä–µ–∂–∏–º —ç–∫–æ–Ω–æ–º–∏–∏ —Ä–µ—Å—É—Ä—Å–æ–≤' –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö"
            ])

        if self.device_profile.available_ram_gb < 4.0:
            recommendations.extend([
                "‚ö†Ô∏è –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –º–∞–ª–æ –¥–æ—Å—Ç—É–ø–Ω–æ–π –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏ (<4 –ì–ë)",
                "üí° –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ: –≤–∫–ª—é—á–∏—Ç—å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á",
                "üí° –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ int4",
                "üí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: —É–≤–µ–ª–∏—á–∏—Ç—å —Ñ–∞–π–ª –ø–æ–¥–∫–∞—á–∫–∏ –¥–æ 8 –ì–ë"
            ])

        if self.device_profile.capability == DeviceCapability.INTEGRATED_GPU:
            recommendations.extend([
                "‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥—Ä–∞—Ñ–∏–∫–∞ ‚Äî –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞",
                "üí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –¥—Ä–∞–π–≤–µ—Ä—ã –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤–µ—Ä—Å–∏–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏",
                "üí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é —Ä–∞–±–æ—Ç—É –¥–æ 2 –º–æ–¥–µ–ª–µ–π –ò–ò",
                "üí° –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±–ª–∞—á–Ω—ã–µ –ò–ò-—Å–µ—Ä–≤–∏—Å—ã –¥–ª—è —Ç—è–∂–µ–ª—ã—Ö –∑–∞–¥–∞—á (–æ–ø–ª–∞—á–∏–≤–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ)"
            ])

        return {
            "device_profile": {
                "capability": self.device_profile.capability.value,
                "ram_total_gb": round(self.device_profile.total_ram_gb, 1),
                "ram_available_gb": round(self.device_profile.available_ram_gb, 1),
                "has_gpu": self.device_profile.has_gpu,
                "gpu_vram_gb": round(self.device_profile.gpu_vram_gb, 1) if self.device_profile.gpu_vram_gb else None
            },
            "recommended_variant": self.device_profile.recommended_variant.value,
            "recommendations": recommendations,
            "estimated_performance": self._estimate_performance()
        }

    def _estimate_performance(self) -> Dict[str, str]:
        """–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á"""
        estimates = {}

        if self.device_profile.capability == DeviceCapability.CPU_ONLY:
            estimates = {
                "text_generation": "15-30 —Å–µ–∫ –Ω–∞ 100 —Å–ª–æ–≤",
                "translation": "5-10 —Å–µ–∫ –Ω–∞ –∞–±–∑–∞—Ü",
                "transcription": "2-3x —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏",
                "embedding": "3-5 —Å–µ–∫ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç"
            }
        elif self.device_profile.capability == DeviceCapability.INTEGRATED_GPU:
            estimates = {
                "text_generation": "8-15 —Å–µ–∫ –Ω–∞ 100 —Å–ª–æ–≤",
                "translation": "3-6 —Å–µ–∫ –Ω–∞ –∞–±–∑–∞—Ü",
                "transcription": "1.5x —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏",
                "embedding": "2-3 —Å–µ–∫ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç"
            }
        elif self.device_profile.capability == DeviceCapability.MID_RANGE_GPU:
            estimates = {
                "text_generation": "3-6 —Å–µ–∫ –Ω–∞ 100 —Å–ª–æ–≤",
                "translation": "1-2 —Å–µ–∫ –Ω–∞ –∞–±–∑–∞—Ü",
                "transcription": "0.8x —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏",
                "embedding": "0.5-1 —Å–µ–∫ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç"
            }
        else:  # HIGH_END_GPU
            estimates = {
                "text_generation": "1-2 —Å–µ–∫ –Ω–∞ 100 —Å–ª–æ–≤",
                "translation": "0.3-0.5 —Å–µ–∫ –Ω–∞ –∞–±–∑–∞—Ü",
                "transcription": "0.3x —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏",
                "embedding": "0.2-0.3 —Å–µ–∫ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç"
            }

        return estimates

    async def cleanup_memory(self):
        """–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –æ—Ç –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ CUDA –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")

        # –í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π, –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–≤—à–∏—Ö—Å—è –±–æ–ª–µ–µ 15 –º–∏–Ω—É—Ç
        current_time = psutil.time()
        models_to_unload = []

        for model_name, model_info in self.loaded_models.items():
            last_used = model_info.get("last_used", 0)
            if current_time - last_used > 900:  # 15 –º–∏–Ω—É—Ç
                models_to_unload.append(model_name)

        for model_name in models_to_unload:
            del self.loaded_models[model_name]
            logger.info(f"–ú–æ–¥–µ–ª—å {model_name} –≤—ã–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ –ø–∞–º—è—Ç–∏ –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤")

    def health_check(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π"""
        return {
            "device_profile": self.device_profile.capability.value,
            "available_ram_gb": round(self.device_profile.available_ram_gb, 1),
            "loaded_models": list(self.loaded_models.keys()),
            "gpu_available": self.device_profile.has_gpu,
            "gpu_vram_gb": round(self.device_profile.gpu_vram_gb, 1) if self.device_profile.gpu_vram_gb else None,
            "recommendations": self.get_performance_recommendations()["recommendations"][:3]  # –ü–µ—Ä–≤—ã–µ 3 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        }