import os
import json
import shutil
import tarfile
import hashlib
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from enum import Enum
import boto3  # –î–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å S3/Yandex Object Storage
from core.security.encryption_engine import EncryptionEngine


class BackupType(Enum):
    FULL = "full"
    INCREMENTAL = "incremental"
    DIFFERENTIAL = "differential"


class BackupRetentionPolicy:
    """
    –ü–æ–ª–∏—Ç–∏–∫–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ä–æ—Ç–∞—Ü–∏–µ–π.
    """

    def __init__(self, config_path: str = "backup/backup_config.json"):
        self.config_path = Path(config_path)
        self.config = self._load_config()

    def _load_config(self) -> Dict:
        default_config = {
            "retention": {
                "daily": {"count": 7, "keep_for_days": 7},
                "weekly": {"count": 4, "keep_for_days": 28},
                "monthly": {"count": 12, "keep_for_days": 365},
                "yearly": {"count": 5, "keep_for_days": 1825}
            },
            "compression": {
                "enabled": True,
                "algorithm": "gzip",  # gzip, bzip2, xz
                "level": 6  # 1-9 –¥–ª—è gzip
            },
            "encryption": {
                "enabled": True,
                "algorithm": "AES-256-GCM"
            },
            "cloud_sync": {
                "enabled": False,
                "provider": "yandex",  # yandex, aws, google
                "bucket": "ai-freelance-backups",
                "region": "ru-central1",
                "sync_after_backup": True
            },
            "verification": {
                "enabled": True,
                "verify_checksum": True,
                "test_restore": False  # –¢–µ—Å—Ç–æ–≤–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ (—Ä–µ—Å—É—Ä—Å–æ—ë–º–∫–æ)
            }
        }

        if self.config_path.exists():
            with open(self.config_path) as f:
                user_config = json.load(f)
                # –ú–µ—Ä–∂–∏–º —Å –¥–µ—Ñ–æ–ª—Ç–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
                self._deep_merge(default_config, user_config)

        return default_config

    def _deep_merge(self, base: Dict, update: Dict):
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ —Å–ª–æ–≤–∞—Ä–µ–π"""
        for key, value in update.items():
            if isinstance(value, dict) and key in base and isinstance(base[key], dict):
                self._deep_merge(base[key], value)
            else:
                base[key] = value

    def get_backup_schedule(self) -> Dict[str, List[str]]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è –±—ç–∫–∞–ø–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª–∏—Ç–∏–∫.
        """
        schedule_path = Path("backup/backup_schedule.json")
        default_schedule = {
            "daily": ["02:00"],
            "weekly": ["sunday 03:00"],
            "monthly": ["1st 04:00"],
            "yearly": ["january-1st 05:00"]
        }

        if schedule_path.exists():
            with open(schedule_path) as f:
                return json.load(f)

        return default_schedule

    def should_create_backup(self, backup_type: str, last_backup_time: Optional[datetime]) -> bool:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å–æ–∑–¥–∞–Ω–∏—è –±—ç–∫–∞–ø–∞ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞.
        """
        now = datetime.utcnow()
        policy = self.config["retention"][backup_type]

        if last_backup_time is None:
            return True

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        if backup_type == "daily":
            return (now - last_backup_time).days >= 1
        elif backup_type == "weekly":
            return (now - last_backup_time).days >= 7
        elif backup_type == "monthly":
            return (now.year > last_backup_time.year or
                    (now.year == last_backup_time.year and now.month > last_backup_time.month))
        elif backup_type == "yearly":
            return now.year > last_backup_time.year

        return False

    def cleanup_old_backups(self, backup_type: str):
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –±—ç–∫–∞–ø–æ–≤ —Å–æ–≥–ª–∞—Å–Ω–æ –ø–æ–ª–∏—Ç–∏–∫–µ —Ö—Ä–∞–Ω–µ–Ω–∏—è.
        """
        backup_dir = Path(f"backup/automatic/{backup_type}")
        if not backup_dir.exists():
            return

        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –±—ç–∫–∞–ø–æ–≤
        backups = sorted(
            [p for p in backup_dir.iterdir() if p.is_dir() or p.suffix in ('.tar', '.tar.gz', '.tar.bz2', '.tar.xz')],
            key=lambda p: p.stat().st_mtime,
            reverse=True
        )

        policy = self.config["retention"][backup_type]
        max_count = policy["count"]

        # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –±—ç–∫–∞–ø–æ–≤
        to_delete = backups[max_count:]
        deleted = 0

        for backup in to_delete:
            try:
                if backup.is_dir():
                    shutil.rmtree(backup)
                else:
                    backup.unlink()
                deleted += 1
                print(f"üóëÔ∏è  –£–¥–∞–ª—ë–Ω —Å—Ç–∞—Ä—ã–π {backup_type} –±—ç–∫–∞–ø: {backup.name}")
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è {backup}: {e}")

        if deleted > 0:
            print(f"‚úÖ –û—á–∏—â–µ–Ω–æ {deleted} —Å—Ç–∞—Ä—ã—Ö {backup_type} –±—ç–∫–∞–ø–æ–≤")


class UnifiedBackupManager:
    """
    –ï–¥–∏–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –±—ç–∫–∞–ø–æ–≤.
    –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç:
    - –ï–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ø–æ–ª–Ω—ã—Ö/–∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –±—ç–∫–∞–ø–æ–≤
    - –®–∏—Ñ—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    - –°–∂–∞—Ç–∏–µ
    - –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏
    - –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é —Å –æ–±–ª–∞–∫–æ–º
    """

    def __init__(self, config_path: str = "backup/backup_config.json"):
        self.policy = BackupRetentionPolicy(config_path)
        self.encryption_engine = EncryptionEngine() if self.policy.config["encryption"]["enabled"] else None
        self.backup_root = Path("backup/automatic")
        self.manual_root = Path("backup/manual")
        self.metadata_root = Path("data/backup_metadata")
        self.metadata_root.mkdir(parents=True, exist_ok=True)

    def create_backup(self, backup_type: BackupType, name: Optional[str] = None) -> Dict:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞.

        :param backup_type: –¢–∏–ø –±—ç–∫–∞–ø–∞ (–ø–æ–ª–Ω—ã–π/–∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π)
        :param name: –ö–∞—Å—Ç–æ–º–Ω–æ–µ –∏–º—è (–¥–ª—è —Ä—É—á–Ω—ã—Ö –±—ç–∫–∞–ø–æ–≤)
        :return: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –±—ç–∫–∞–ø–∞
        """
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        backup_name = name or f"{backup_type.value}_{timestamp}"

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è
        if name:  # –†—É—á–Ω–æ–π –±—ç–∫–∞–ø
            backup_dir = self.manual_root / backup_name
        else:  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –±—ç–∫–∞–ø
            backup_dir = self.backup_root / backup_type.value / backup_name

        backup_dir.mkdir(parents=True, exist_ok=True)

        print(f"üíæ –°–æ–∑–¥–∞–Ω–∏–µ {backup_type.value} –±—ç–∫–∞–ø–∞: {backup_name}")

        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –±—ç–∫–∞–ø–∞
        metadata = {
            "backup_id": hashlib.sha256(f"{backup_name}_{timestamp}".encode()).hexdigest()[:16],
            "name": backup_name,
            "type": backup_type.value,
            "created_at": datetime.utcnow().isoformat(),
            "version": "2.0",
            "encrypted": bool(self.encryption_engine),
            "compression": self.policy.config["compression"]["enabled"],
            "sources": [],
            "checksums": {},
            "size_bytes": 0
        }

        total_size = 0

        # 1. –ë—ç–∫–∞–ø –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        db_backup_path = self._backup_database(backup_dir)
        if db_backup_path:
            metadata["sources"].append("database")
            metadata["checksums"]["database"] = self._calculate_checksum(db_backup_path)
            total_size += db_backup_path.stat().st_size

        # 2. –ë—ç–∫–∞–ø –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
        data_backup_path = self._backup_application_data(backup_dir)
        if data_backup_path:
            metadata["sources"].append("application_data")
            metadata["checksums"]["application_data"] = self._calculate_checksum(data_backup_path)
            total_size += data_backup_path.stat().st_size

        # 3. –ë—ç–∫–∞–ø –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
        config_backup_path = self._backup_configurations(backup_dir)
        if config_backup_path:
            metadata["sources"].append("configurations")
            metadata["checksums"]["configurations"] = self._calculate_checksum(config_backup_path)
            total_size += config_backup_path.stat().st_size

        # 4. –ë—ç–∫–∞–ø –º–æ–¥–µ–ª–µ–π –ò–ò (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ ‚Äî –∏–∑-–∑–∞ –±–æ–ª—å—à–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞)
        if backup_type == BackupType.FULL:
            models_backup_path = self._backup_ai_models(backup_dir)
            if models_backup_path:
                metadata["sources"].append("ai_models")
                metadata["checksums"]["ai_models"] = self._calculate_checksum(models_backup_path)
                total_size += models_backup_path.stat().st_size

        metadata["size_bytes"] = total_size
        metadata["size_human"] = self._human_size(total_size)

        # –®–∏—Ñ—Ä–æ–≤–∞–Ω–∏–µ –±—ç–∫–∞–ø–∞ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
        if self.encryption_engine:
            print(" üîí –®–∏—Ñ—Ä–æ–≤–∞–Ω–∏–µ –±—ç–∫–∞–ø–∞...")
            encrypted_path = self._encrypt_backup_directory(backup_dir)
            metadata["encrypted_path"] = str(encrypted_path)

        # –°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞
        if self.policy.config["compression"]["enabled"]:
            print(" üì¶ –°–∂–∞—Ç–∏–µ –±—ç–∫–∞–ø–∞...")
            archive_path = self._create_compressed_archive(backup_dir, backup_name)
            metadata["archive_path"] = str(archive_path)
            metadata["checksums"]["archive"] = self._calculate_checksum(archive_path)

        # –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏
        if self.policy.config["verification"]["enabled"]:
            print(" ‚úÖ –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏...")
            self._verify_backup_integrity(metadata, backup_dir)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        metadata_path = backup_dir / "backup_metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)

        # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Å –æ–±–ª–∞–∫–æ–º
        if self.policy.config["cloud_sync"]["enabled"]:
            print(" ‚òÅÔ∏è  –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Å –æ–±–ª–∞–∫–æ–º...")
            self._sync_to_cloud(backup_dir, metadata)

        # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –±—ç–∫–∞–ø–æ–≤
        if not name:  # –¢–æ–ª—å–∫–æ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –±—ç–∫–∞–ø–æ–≤
            self.policy.cleanup_old_backups(backup_type.value)

        print(f"‚úÖ –ë—ç–∫–∞–ø —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω: {backup_dir}")
        print(f"üìä –†–∞–∑–º–µ—Ä: {metadata['size_human']}")

        return metadata

    def _backup_database(self, backup_dir: Path) -> Optional[Path]:
        """–†–µ–∑–µ—Ä–≤–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ pg_dump –∏–ª–∏ –∞–Ω–∞–ª–æ–≥–∞ –¥–ª—è –≤–∞—à–µ–π –ë–î
            import subprocess

            db_dump_path = backup_dir / "database_dump.sql"

            # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            db_config = json.loads(Path("config/database.json").read_text())
            conn_str = f"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['name']}"

            subprocess.run([
                "pg_dump",
                "--format=custom",
                f"--file={db_dump_path}",
                conn_str
            ], check=True, capture_output=True)

            print("   üíæ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
            return db_dump_path

        except Exception as e:
            print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –±—ç–∫–∞–ø–∞ –ë–î: {e}")
            return None

    def _backup_application_data(self, backup_dir: Path) -> Path:
        """–†–µ–∑–µ—Ä–≤–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è (–∫–ª–∏–µ–Ω—Ç—ã, –∑–∞–∫–∞–∑—ã, —Ñ–∏–Ω–∞–Ω—Å—ã)"""
        data_sources = [
            ("clients", "data/clients"),
            ("jobs", "data/jobs"),
            ("finances", "data/finances"),
            ("projects", "data/projects"),
            ("conversations", "data/conversations"),
            ("stats", "data/stats"),
            ("settings", "data/settings")
        ]

        data_backup_dir = backup_dir / "application_data"
        data_backup_dir.mkdir(exist_ok=True)

        for name, source in data_sources:
            source_path = Path(source)
            if source_path.exists():
                dest_path = data_backup_dir / name
                if source_path.is_dir():
                    shutil.copytree(source_path, dest_path, dirs_exist_ok=True)
                else:
                    shutil.copy2(source_path, dest_path)

        print("   üíæ –î–∞–Ω–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
        return data_backup_dir

    def _backup_configurations(self, backup_dir: Path) -> Path:
        """–†–µ–∑–µ—Ä–≤–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π"""
        config_sources = [
            "config",
            "ai/configs",
            ".env",  # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
            "backup/backup_config.json",
            "backup/backup_schedule.json"
        ]

        config_backup_dir = backup_dir / "configurations"
        config_backup_dir.mkdir(exist_ok=True)

        for source in config_sources:
            source_path = Path(source)
            if source_path.exists():
                if source_path.is_dir():
                    shutil.copytree(source_path, config_backup_dir / source_path.name, dirs_exist_ok=True)
                else:
                    shutil.copy2(source_path, config_backup_dir)

        print("   ‚öôÔ∏è  –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
        return config_backup_dir

    def _backup_ai_models(self, backup_dir: Path) -> Optional[Path]:
        """–†–µ–∑–µ—Ä–≤–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ò–ò (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–ª–Ω—ã—Ö –±—ç–∫–∞–ø–æ–≤)"""
        models_path = Path("ai/models")
        if not models_path.exists():
            return None

        models_backup_dir = backup_dir / "ai_models"
        models_backup_dir.mkdir(exist_ok=True)

        # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏ –∫–æ–Ω—Ñ–∏–≥–æ–≤ (—Å–∞–º–∏ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–º–∏)
        # –î–ª—è –ø–æ–ª–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–Ω–µ—à–Ω–µ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∏–ª–∏ —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏–µ —Å—Å—ã–ª–∫–∏
        for model_dir in models_path.iterdir():
            if model_dir.is_dir():
                # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏
                for file in model_dir.glob("*config.json"):
                    shutil.copy2(file, models_backup_dir / f"{model_dir.name}_{file.name}")
                # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
                for file in model_dir.glob("*tokenizer*"):
                    if file.is_file():
                        shutil.copy2(file, models_backup_dir / f"{model_dir.name}_{file.name}")

        print("   ü§ñ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–µ–π –ò–ò —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
        return models_backup_dir

    def _encrypt_backup_directory(self, backup_dir: Path) -> Path:
        """–®–∏—Ñ—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –±—ç–∫–∞–ø–∞"""
        if not self.encryption_engine:
            return backup_dir

        encrypted_dir = backup_dir.with_suffix(".encrypted")

        for file_path in backup_dir.rglob("*"):
            if file_path.is_file():
                relative_path = file_path.relative_to(backup_dir)
                encrypted_path = encrypted_dir / relative_path
                encrypted_path.parent.mkdir(parents=True, exist_ok=True)

                with open(file_path, 'rb') as f:
                    data = f.read()

                encrypted_data = self.encryption_engine.encrypt(data)

                with open(encrypted_path, 'wb') as f:
                    f.write(encrypted_data)

        # –£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ—à–∏—Ñ—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ —à–∏—Ñ—Ä–æ–≤–∞–Ω–∏—è
        shutil.rmtree(backup_dir)

        return encrypted_dir

    def _create_compressed_archive(self, source_dir: Path, archive_name: str) -> Path:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∂–∞—Ç–æ–≥–æ –∞—Ä—Ö–∏–≤–∞ –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –±—ç–∫–∞–ø–∞"""
        compression = self.policy.config["compression"]
        archive_path = source_dir.with_suffix(f".tar.{compression['algorithm'][-2:]}")

        mode = 'w:gz' if compression['algorithm'] == 'gzip' else \
            'w:bz2' if compression['algorithm'] == 'bzip2' else \
                'w:xz'

        with tarfile.open(archive_path, mode, compresslevel=compression.get('level', 6)) as tar:
            tar.add(source_dir, arcname=archive_name)

        # –£–¥–∞–ª–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø–æ—Å–ª–µ –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏
        if source_dir.exists() and source_dir.is_dir():
            shutil.rmtree(source_dir)

        return archive_path

    def _calculate_checksum(self, path: Path) -> str:
        """–†–∞—Å—á—ë—Ç –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ–π —Å—É–º–º—ã —Ñ–∞–π–ª–∞"""
        hash_sha256 = hashlib.sha256()
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()

    def _verify_backup_integrity(self, metadata: Dict, backup_dir: Path):
        """–í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –±—ç–∫–∞–ø–∞"""
        for source, expected_checksum in metadata["checksums"].items():
            if source == "archive":
                path = Path(metadata["archive_path"])
            else:
                path = backup_dir / f"{source}_dump.sql" if source == "database" else backup_dir / source

            if path.exists():
                actual_checksum = self._calculate_checksum(path)
                if actual_checksum != expected_checksum:
                    raise ValueError(
                        f"–ù–∞—Ä—É—à–µ–Ω–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –±—ç–∫–∞–ø–∞ –¥–ª—è {source}: {actual_checksum} != {expected_checksum}")

        print("   ‚úÖ –¶–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –±—ç–∫–∞–ø–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞")

    def _sync_to_cloud(self, backup_dir: Path, metadata: Dict):
        """–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –±—ç–∫–∞–ø–∞ —Å –æ–±–ª–∞—á–Ω—ã–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ–º"""
        cloud_config = self.policy.config["cloud_sync"]

        if cloud_config["provider"] == "yandex":
            session = boto3.session.Session()
            s3 = session.client(
                service_name='s3',
                endpoint_url='https://storage.yandexcloud.net',
                aws_access_key_id=os.environ.get('YC_ACCESS_KEY_ID'),
                aws_secret_access_key=os.environ.get('YC_SECRET_ACCESS_KEY'),
                region_name=cloud_config["region"]
            )

            # –ó–∞–≥—Ä—É–∑–∫–∞ –∞—Ä—Ö–∏–≤–∞
            archive_path = Path(metadata["archive_path"])
            s3_key = f"backups/{metadata['type']}/{archive_path.name}"

            s3.upload_file(
                Filename=str(archive_path),
                Bucket=cloud_config["bucket"],
                Key=s3_key,
                ExtraArgs={
                    'Metadata': {
                        'backup-id': metadata['backup_id'],
                        'created-at': metadata['created_at'],
                        'size-bytes': str(metadata['size_bytes'])
                    }
                }
            )

            print(f"   ‚òÅÔ∏è  –ë—ç–∫–∞–ø –∑–∞–≥—Ä—É–∂–µ–Ω –≤ Yandex Object Storage: {s3_key}")

        # –î–æ–±–∞–≤–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É –¥—Ä—É–≥–∏—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏

    def _human_size(self, size_bytes: int) -> str:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –±–∞–π—Ç–æ–≤ –≤ —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç"""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.2f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.2f} PB"

    def restore_backup(self, backup_id: str, target_dir: Optional[str] = None):
        """
        –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –∏–∑ –±—ç–∫–∞–ø–∞.
        """
        print(f"üîÑ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑ –±—ç–∫–∞–ø–∞: {backup_id}")
        # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è (—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞—è –ø—Ä–æ—Ü–µ—Å—Å—É –±—ç–∫–∞–ø–∞)
        # 1. –ü–æ–∏—Å–∫ –±—ç–∫–∞–ø–∞ –ø–æ ID
        # 2. –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –∞—Ä—Ö–∏–≤–∞
        # 3. –†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ (–µ—Å–ª–∏ –∑–∞—à–∏—Ñ—Ä–æ–≤–∞–Ω)
        # 4. –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ë–î
        # 5. –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
        # 6. –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è
        raise NotImplementedError("–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏")

    def list_backups(self, backup_type: Optional[str] = None) -> List[Dict]:
        """
        –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –±—ç–∫–∞–ø–æ–≤.
        """
        backups = []

        for bt in ["daily", "weekly", "monthly", "yearly", "manual"]:
            if backup_type and bt != backup_type:
                continue

            dir_path = self.backup_root / bt if bt != "manual" else self.manual_root
            if not dir_path.exists():
                continue

            for backup_item in dir_path.iterdir():
                metadata_path = backup_item / "backup_metadata.json" if backup_item.is_dir() else None
                if not metadata_path or not metadata_path.exists():
                    # –ü–æ–∏—Å–∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤ –∞—Ä—Ö–∏–≤–µ –∏–ª–∏ —Ä—è–¥–æ–º —Å –Ω–∏–º
                    metadata_path = backup_item.parent / f"{backup_item.stem}_metadata.json"

                if metadata_path and metadata_path.exists():
                    with open(metadata_path) as f:
                        metadata = json.load(f)
                        metadata["location"] = str(backup_item)
                        backups.append(metadata)

        return sorted(backups, key=lambda x: x["created_at"], reverse=True)


# CLI-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
def backup_cli():
    import argparse

    parser = argparse.ArgumentParser(description="–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è")
    parser.add_argument("action", choices=["create", "list", "restore", "cleanup"], help="–î–µ–π—Å—Ç–≤–∏–µ")
    parser.add_argument("--type", choices=["full", "incremental"], default="full", help="–¢–∏–ø –±—ç–∫–∞–ø–∞")
    parser.add_argument("--name", help="–ò–º—è –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –±—ç–∫–∞–ø–∞")
    parser.add_argument("--backup-id", help="ID –±—ç–∫–∞–ø–∞ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è")
    parser.add_argument("--target-dir", help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è")

    args = parser.parse_args()
    manager = UnifiedBackupManager()

    if args.action == "create":
        backup_type = BackupType.FULL if args.type == "full" else BackupType.INCREMENTAL
        manager.create_backup(backup_type, args.name)

    elif args.action == "list":
        backups = manager.list_backups()
        for b in backups:
            print(f"{b['created_at'][:10]} | {b['type']:10} | {b['size_human']:10} | {b['name']}")

    elif args.action == "restore":
        if not args.backup_id:
            raise ValueError("--backup-id –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è")
        manager.restore_backup(args.backup_id, args.target_dir)

    elif args.action == "cleanup":
        for bt in ["daily", "weekly", "monthly"]:
            manager.policy.cleanup_old_backups(bt)


if __name__ == "__main__":
    backup_cli()