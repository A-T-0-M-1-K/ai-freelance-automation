# AI_FREELANCE_AUTOMATION/scripts/maintenance/backup_system.py
"""
Backup System ‚Äî –Ω–∞–¥–µ–∂–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ø–æ–ª–Ω—ã—Ö –∏ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –±—ç–∫–∞–ø–æ–≤
- –®–∏—Ñ—Ä–æ–≤–∞–Ω–∏–µ –±—ç–∫–∞–ø–æ–≤ —á–µ—Ä–µ–∑ AdvancedCryptoSystem
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å unified_config_manager
- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é —Å–∏—Å—Ç–µ–º—É –ª–æ–≥–æ–≤
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å—Ä–æ–∫–æ–º —Ö—Ä–∞–Ω–µ–Ω–∏—è (retention)
- –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É—Ç–µ–π (–∑–∞—â–∏—Ç–∞ –æ—Ç path traversal)
- –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –±—ç–∫–∞–ø–∞

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç:
- core.config.unified_config_manager.UnifiedConfigManager
- core.security.advanced_crypto_system.AdvancedCryptoSystem
- logging –∏–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
"""

import os
import json
import shutil
import hashlib
import logging
import tarfile
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional, Dict, Any

# –ò–º–ø–æ—Ä—Ç—ã —è–¥—Ä–∞ (—á–µ—Ä–µ–∑ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –ø—É—Ç–∏, –∫–∞–∫ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –ø—Ä–æ–µ–∫—Ç–∞)
try:
    from core.config.unified_config_manager import UnifiedConfigManager
    from core.security.advanced_crypto_system import AdvancedCryptoSystem
except ImportError as e:
    raise ImportError(
        "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —è–¥—Ä–∞. "
        "–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∏–∑ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞ –∏–ª–∏ PYTHONPATH –Ω–∞—Å—Ç—Ä–æ–µ–Ω –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ."
    ) from e


class BackupSystem:
    def __init__(self, config: Optional[UnifiedConfigManager] = None):
        self.config = config or UnifiedConfigManager()
        self.crypto = AdvancedCryptoSystem()
        self.logger = logging.getLogger("BackupSystem")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –∏–∑ config/backup_config.json
        backup_cfg = self.config.get("backup", {})
        self.backup_root = Path(backup_cfg.get("backup_root", "backup/automatic"))
        self.retention_days = backup_cfg.get("retention_days", 30)
        self.encrypt_backups = backup_cfg.get("encrypt", True)
        self.compression_level = backup_cfg.get("compression_level", 6)

        self.backup_root.mkdir(parents=True, exist_ok=True)
        self.logger.info(f"Intialized BackupSystem with root: {self.backup_root}")

    def create_backup(self, backup_type: str = "incremental") -> str:
        """
        –°–æ–∑–¥–∞–µ—Ç —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é.

        :param backup_type: "full" –∏–ª–∏ "incremental"
        :return: –ø—É—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É –∞—Ä—Ö–∏–≤—É
        """
        if backup_type not in ("full", "incremental"):
            raise ValueError("backup_type must be 'full' or 'incremental'")

        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        backup_dir = self.backup_root / backup_type / timestamp
        backup_dir.mkdir(parents=True, exist_ok=True)

        data_sources = self._get_data_sources()
        manifest = {
            "type": backup_type,
            "timestamp": timestamp,
            "sources": [],
            "hashes": {},
            "encrypted": self.encrypt_backups
        }

        for src_name, src_path in data_sources.items():
            if not Path(src_path).exists():
                self.logger.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫: {src_path}")
                continue

            self.logger.info(f"–ë—ç–∫–∞–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {src_name} ‚Üí {src_path}")
            dest_path = backup_dir / src_name
            shutil.copytree(src_path, dest_path, dirs_exist_ok=True)

            # –•—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏
            file_hash = self._calculate_directory_hash(dest_path)
            manifest["sources"].append(src_name)
            manifest["hashes"][src_name] = file_hash

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞
        manifest_path = backup_dir / "manifest.json"
        with open(manifest_path, "w", encoding="utf-8") as f:
            json.dump(manifest, f, indent=2, ensure_ascii=False)

        # –ê—Ä—Ö–∏–≤–∞—Ü–∏—è
        archive_name = f"{backup_type}_{timestamp}.tar.gz"
        archive_path = self.backup_root / backup_type / archive_name

        with tarfile.open(archive_path, "w:gz", compresslevel=self.compression_level) as tar:
            tar.add(backup_dir, arcname=timestamp)

        # –®–∏—Ñ—Ä–æ–≤–∞–Ω–∏–µ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
        if self.encrypt_backups:
            encrypted_path = archive_path.with_suffix(".tar.gz.enc")
            self.crypto.encrypt_file(archive_path, encrypted_path)
            archive_path.unlink()  # –£–¥–∞–ª—è–µ–º –Ω–µ–∑–∞—à–∏—Ñ—Ä–æ–≤–∞–Ω–Ω—ã–π –∞—Ä—Ö–∏–≤
            archive_path = encrypted_path

        # –£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        shutil.rmtree(backup_dir)

        self.logger.info(f"‚úÖ –ë—ç–∫–∞–ø —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω: {archive_path}")
        return str(archive_path)

    def _get_data_sources(self) -> Dict[str, str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –ø—É—Ç–µ–π –∫ –¥–∞–Ω–Ω—ã–º, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –±—ç–∫–∞–ø–∏—Ç—å."""
        return {
            "data": "data/",
            "config": "config/",
            "ai_models": "ai/models/",
            "logs": "logs/",
            "templates": "templates/"
        }

    def _calculate_directory_hash(self, directory: Path) -> str:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç SHA-256 —Ö—ç—à –≤—Å–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
        hash_obj = hashlib.sha256()
        for root, _, files in sorted(os.walk(directory)):
            for fname in sorted(files):
                fpath = Path(root) / fname
                try:
                    with open(fpath, "rb") as f:
                        while chunk := f.read(8192):
                            hash_obj.update(chunk)
                except OSError as e:
                    self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª {fpath}: {e}")
        return hash_obj.hexdigest()

    def cleanup_old_backups(self):
        """–£–¥–∞–ª—è–µ—Ç –±—ç–∫–∞–ø—ã —Å—Ç–∞—Ä—à–µ retention_days."""
        cutoff = datetime.utcnow() - timedelta(days=self.retention_days)
        deleted = 0

        for backup_type in ("full", "incremental"):
            type_dir = self.backup_root / backup_type
            if not type_dir.exists():
                continue

            for item in type_dir.iterdir():
                if item.is_file() and item.suffix in (".gz", ".enc"):
                    try:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º timestamp –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞: full_20260124_120000.tar.gz.enc
                        name_parts = item.stem.split("_")
                        if len(name_parts) < 3:
                            continue
                        date_str = name_parts[1] + "_" + name_parts[2]
                        backup_time = datetime.strptime(date_str, "%Y%m%d_%H%M%S")
                        if backup_time < cutoff:
                            item.unlink()
                            self.logger.info(f"üóëÔ∏è –£–¥–∞–ª—ë–Ω —Å—Ç–∞—Ä—ã–π –±—ç–∫–∞–ø: {item}")
                            deleted += 1
                    except (ValueError, IndexError):
                        self.logger.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω —Ñ–∞–π–ª —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º –∏–º–µ–Ω–µ–º: {item}")

        self.logger.info(f"üßπ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: —É–¥–∞–ª–µ–Ω–æ {deleted} —Å—Ç–∞—Ä—ã—Ö –±—ç–∫–∞–ø–æ–≤.")

    def verify_backup(self, backup_path: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –±—ç–∫–∞–ø–∞ (—Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ + —Ö—ç—à)."""
        backup_path = Path(backup_path)
        if not backup_path.exists():
            self.logger.error(f"–ë—ç–∫–∞–ø –Ω–µ –Ω–∞–π–¥–µ–Ω: {backup_path}")
            return False

        temp_dir = Path("temp/backup_verify")
        temp_dir.mkdir(parents=True, exist_ok=True)
        try:
            # –†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if backup_path.suffix == ".enc":
                decrypted_path = temp_dir / backup_path.with_suffix("").name
                self.crypto.decrypt_file(backup_path, decrypted_path)
                archive_to_extract = decrypted_path
            else:
                archive_to_extract = backup_path

            # –†–∞—Å–ø–∞–∫–æ–≤–∫–∞
            extract_dir = temp_dir / "extracted"
            with tarfile.open(archive_to_extract, "r:gz") as tar:
                tar.extractall(extract_dir)

            # –ü–æ–∏—Å–∫ manifest.json
            manifest_file = None
            for root, _, files in os.walk(extract_dir):
                if "manifest.json" in files:
                    manifest_file = Path(root) / "manifest.json"
                    break

            if not manifest_file:
                self.logger.error("–ú–∞–Ω–∏—Ñ–µ—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –±—ç–∫–∞–ø–µ")
                return False

            with open(manifest_file, "r", encoding="utf-8") as f:
                manifest = json.load(f)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ö—ç—à–µ–π
            for source in manifest["sources"]:
                dir_path = extract_dir / list(extract_dir.iterdir())[0] / source
                if not dir_path.exists():
                    self.logger.error(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –≤ –±—ç–∫–∞–ø–µ: {source}")
                    return False
                current_hash = self._calculate_directory_hash(dir_path)
                expected_hash = manifest["hashes"][source]
                if current_hash != expected_hash:
                    self.logger.error(f"–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ö—ç—à–∞ –¥–ª—è {source}")
                    return False

            self.logger.info("‚úÖ –ë—ç–∫–∞–ø –ø—Ä–æ—à—ë–ª –ø—Ä–æ–≤–µ—Ä–∫—É —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏.")
            return True

        finally:
            shutil.rmtree(temp_dir, ignore_errors=True)


def main():
    """CLI-—Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞."""
    import argparse

    parser = argparse.ArgumentParser(description="–°–∏—Å—Ç–µ–º–∞ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è AI Freelance Automation")
    parser.add_argument("--type", choices=["full", "incremental"], default="incremental")
    parser.add_argument("--verify", type=str, help="–ü—É—Ç—å –∫ –±—ç–∫–∞–ø—É –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏")
    parser.add_argument("--cleanup", action="store_true", help="–í—ã–ø–æ–ª–Ω–∏—Ç—å –æ—á–∏—Å—Ç–∫—É —Å—Ç–∞—Ä—ã—Ö –±—ç–∫–∞–ø–æ–≤")

    args = parser.parse_args()

    logging.basicConfig(level=logging.INFO)

    backup_system = BackupSystem()

    if args.verify:
        success = backup_system.verify_backup(args.verify)
        exit(0 if success else 1)

    if args.cleanup:
        backup_system.cleanup_old_backups()

    if not args.verify and not args.cleanup:
        backup_system.create_backup(args.type)


if __name__ == "__main__":
    main()