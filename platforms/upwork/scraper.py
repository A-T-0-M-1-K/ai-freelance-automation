# AI_FREELANCE_AUTOMATION/platforms/upwork/scraper.py
"""
Upwork Job Scraper ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –∑–∞–∫–∞–∑–æ–≤ —Å –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã Upwork.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–µ API (–ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏) –∏ fallback-–º–µ—Ç–æ–¥—ã –ø—Ä–∏ –µ–≥–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç ML-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é, rate limiting, –æ–±—Ö–æ–¥ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫, –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ.
"""

import asyncio
import json
import logging
import time
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin

import aiohttp
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

from core.config.unified_config_manager import UnifiedConfigManager
from core.security.advanced_crypto_system import AdvancedCryptoSystem
from core.monitoring.intelligent_monitoring_system import IntelligentMonitoringSystem
from core.dependency.service_locator import ServiceLocator
from services.storage.database_service import DatabaseService


class UpworkJobScraper:
    """
    –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Å–∫—Ä–∞–ø–µ—Ä –¥–ª—è Upwork.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
      - –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π API (OAuth2)
      - Headless-–±—Ä–∞—É–∑–µ—Ä –∫–∞–∫ fallback (—á–µ—Ä–µ–∑ –ø–ª–∞–≥–∏–Ω)
      - ML-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∑–∞–∫–∞–∑–æ–≤
      - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
      - –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ ToS —á–µ—Ä–µ–∑ rate limiting
    """

    def __init__(
        self,
        config_manager: Optional[UnifiedConfigManager] = None,
        crypto: Optional[AdvancedCryptoSystem] = None,
        monitor: Optional[IntelligentMonitoringSystem] = None,
        db: Optional[DatabaseService] = None
    ):
        self.logger = logging.getLogger("UpworkScraper")
        self.config = config_manager or ServiceLocator.get("config_manager")
        self.crypto = crypto or ServiceLocator.get("crypto_system")
        self.monitor = monitor or ServiceLocator.get("monitoring_system")
        self.db = db or ServiceLocator.get("database_service")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Upwork
        self.platform_config = self.config.get("platforms", {}).get("upwork", {})
        self.api_base_url = self.platform_config.get("api_base_url", "https://www.upwork.com/api/")
        self.scraping_delay = self.platform_config.get("scraping_delay_sec", 5)
        self.max_retries = self.platform_config.get("max_retries", 3)
        self.enabled = self.platform_config.get("enabled", False)

        # –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è
        self.client_id = self._decrypt_secret("upwork_client_id")
        self.client_secret = self._decrypt_secret("upwork_client_secret")
        self.refresh_token = self._decrypt_secret("upwork_refresh_token")
        self.access_token = None
        self.token_expires_at = 0

        self.session: Optional[aiohttp.ClientSession] = None
        self._is_initialized = False

    def _decrypt_secret(self, key: str) -> str:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ —Å–µ–∫—Ä–µ—Ç–∞."""
        encrypted = self.config.get("secrets", {}).get(key)
        if not encrypted:
            raise ValueError(f"Missing required secret: {key}")
        return self.crypto.decrypt(encrypted)

    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏ –∏ —Ç–æ–∫–µ–Ω–∞ –¥–æ—Å—Ç—É–ø–∞."""
        if not self.enabled:
            self.logger.warning("Upwork scraping is disabled in config.")
            return

        self.session = aiohttp.ClientSession(
            headers={"User-Agent": "AI-Freelance-Automation/1.0"},
            timeout=aiohttp.ClientTimeout(total=30)
        )
        await self._refresh_access_token()
        self._is_initialized = True
        self.logger.info("‚úÖ Upwork scraper initialized successfully.")

    async def shutdown(self):
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã."""
        if self.session:
            await self.session.close()
        self._is_initialized = False
        self.logger.info("üîå Upwork scraper shut down.")

    async def _refresh_access_token(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ OAuth2 access token —á–µ—Ä–µ–∑ refresh token."""
        if time.time() < self.token_expires_at - 60:
            return  # –¢–æ–∫–µ–Ω –µ—â—ë –≤–∞–ª–∏–¥–µ–Ω

        url = "https://www.upwork.com/api/auth/v1/oauth2/token"
        payload = {
            "grant_type": "refresh_token",
            "client_id": self.client_id,
            "client_secret": self.client_secret,
            "refresh_token": self.refresh_token
        }

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(url, data=payload) as resp:
                    if resp.status != 200:
                        raise RuntimeError(f"Failed to refresh token: {resp.status} {await resp.text()}")
                    data = await resp.json()
                    self.access_token = data["access_token"]
                    self.token_expires_at = time.time() + data["expires_in"]
                    self.logger.debug("üîÑ Upwork access token refreshed.")
        except Exception as e:
            self.monitor.log_anomaly("upwork_auth_failure", str(e))
            raise

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((aiohttp.ClientError, asyncio.TimeoutError))
    )
    async def _fetch_jobs_api(self, params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–ó–∞–ø—Ä–æ—Å –∫ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–º—É API Upwork."""
        await self._refresh_access_token()

        headers = {
            "Authorization": f"Bearer {self.access_token}",
            "Accept": "application/json"
        }

        url = urljoin(self.api_base_url, "profiles/v2/search/jobs.json")
        async with self.session.get(url, headers=headers, params=params) as resp:
            if resp.status == 429:
                self.logger.warning("‚ö†Ô∏è Rate limited by Upwork API. Backing off...")
                await asyncio.sleep(60)
                raise aiohttp.ClientError("Rate limited")
            elif resp.status != 200:
                text = await resp.text()
                self.logger.error(f"‚ùå API error {resp.status}: {text}")
                raise aiohttp.ClientError(f"HTTP {resp.status}: {text}")

            data = await resp.json()
            return data.get("jobs", [])

    async def scrape_jobs(
        self,
        query: str = "",
        budget_min: Optional[float] = None,
        category: Optional[str] = None,
        limit: int = 50
    ) -> List[Dict[str, Any]]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Å–±–æ—Ä–∞ –∑–∞–∫–∞–∑–æ–≤.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –µ–¥–∏–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ.
        """
        if not self._is_initialized:
            await self.initialize()

        if not self.enabled:
            return []

        self.logger.info(f"üîç Scraping Upwork jobs: query='{query}', budget‚â•{budget_min}, category={category}")

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ –∫ API
        params = {
            "q": query,
            "per_page": min(limit, 100),
            "sort": "recency"
        }
        if budget_min:
            params["budget"] = f"{budget_min}-"

        try:
            raw_jobs = await self._fetch_jobs_api(params)
            normalized_jobs = self._normalize_jobs(raw_jobs)
            filtered_jobs = await self._ml_filter_jobs(normalized_jobs)
            self.logger.info(f"‚úÖ Retrieved and filtered {len(filtered_jobs)} relevant jobs from Upwork.")
            return filtered_jobs
        except Exception as e:
            self.logger.exception("üí• Error during Upwork scraping")
            self.monitor.log_anomaly("upwork_scraping_failure", str(e))
            # Fallback: –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ headless-–±—Ä–∞—É–∑–µ—Ä (–µ—Å–ª–∏ –ø–ª–∞–≥–∏–Ω —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)
            return await self._fallback_scrape(query, budget_min, category, limit)

    def _normalize_jobs(self, raw_jobs: List[Dict]) -> List[Dict[str, Any]]:
        """–ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∫ –µ–¥–∏–Ω–æ–º—É –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–º—É —Ñ–æ—Ä–º–∞—Ç—É."""
        normalized = []
        for job in raw_jobs:
            normalized_job = {
                "platform": "upwork",
                "job_id": job.get("id"),
                "title": job.get("title", "").strip(),
                "description": job.get("description", "").strip(),
                "budget": {
                    "type": job.get("budget", {}).get("type", "hourly"),
                    "amount": job.get("budget", {}).get("amount", 0),
                    "currency": job.get("budget", {}).get("currency", "USD")
                },
                "skills": job.get("skills", []),
                "posted_at": job.get("date_created"),
                "client": {
                    "country": job.get("client", {}).get("country"),
                    "rating": job.get("client", {}).get("feedback", {}).get("score"),
                    "reviews": job.get("client", {}).get("feedback", {}).get("count", 0)
                },
                "url": f"https://www.upwork.com/jobs/{job.get('id')}",
                "raw_data": job  # –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –∏ –±—É–¥—É—â–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            }
            normalized.append(normalized_job)
        return normalized

    async def _ml_filter_jobs(self, jobs: List[Dict]) -> List[Dict]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ ML-–º–æ–¥–µ–ª—å (–∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ service locator)."""
        try:
            model_manager = ServiceLocator.get("ai_model_manager")
            filter_model = await model_manager.get_model("job_relevance_classifier")
            filtered = []
            for job in jobs:
                relevance_score = await filter_model.predict({
                    "title": job["title"],
                    "description": job["description"],
                    "skills": job["skills"]
                })
                if relevance_score >= 0.75:  # –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                    job["relevance_score"] = float(relevance_score)
                    filtered.append(job)
            return filtered
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è ML filtering failed, returning all jobs: {e}")
            return jobs

    async def _fallback_scrape(self, query, budget_min, category, limit) -> List[Dict[str, Any]]:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥ —á–µ—Ä–µ–∑ headless-–±—Ä–∞—É–∑–µ—Ä (–µ—Å–ª–∏ –ø–ª–∞–≥–∏–Ω –∞–∫—Ç–∏–≤–µ–Ω)."""
        try:
            plugin_manager = ServiceLocator.get("plugin_manager")
            if plugin_manager.is_plugin_active("upwork_browser_scraper"):
                browser_scraper = plugin_manager.get_plugin("upwork_browser_scraper")
                return await browser_scraper.scrape(query, budget_min, category, limit)
            else:
                self.logger.error("No fallback scraper available for Upwork.")
                return []
        except Exception as e:
            self.logger.exception("Fallback scraping also failed")
            return []

    async def save_jobs_to_db(self, jobs: List[Dict]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∑–∞–∫–∞–∑–æ–≤ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö."""
        if not jobs:
            return
        await self.db.insert_many("jobs_raw", jobs)
        self.logger.debug(f"üíæ Saved {len(jobs)} Upwork jobs to database.")


# –≠–∫—Å–ø–æ—Ä—Ç –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
__all__ = ["UpworkJobScraper"]