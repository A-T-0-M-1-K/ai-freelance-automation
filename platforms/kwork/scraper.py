# platforms/kwork/scraper.py
"""
Kwork.ru Job Scraper ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –∑–∞–∫–∞–∑–æ–≤ —Å –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã Kwork.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–±—Ö–æ–¥ –∞–Ω—Ç–∏-–±–æ—Ç –∑–∞—â–∏—Ç—ã, –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ä–µ–π—Ç-–ª–∏–º–∏—Ç–∏–Ω–≥,
–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Ä–æ—Ç–∞—Ü–∏—é –ø—Ä–æ–∫—Å–∏ –∏ User-Agent, –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–∞–º–æ–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ.

–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å:
- core.config.unified_config_manager
- core.security.advanced_crypto_system
- core.monitoring.intelligent_monitoring_system
- core.performance.intelligent_cache_system
"""

import asyncio
import json
import logging
import random
import time
from typing import Any, Dict, List, Optional, Set
from urllib.parse import urljoin, urlencode

import aiohttp
from bs4 import BeautifulSoup

from core.config.unified_config_manager import UnifiedConfigManager
from core.security.advanced_crypto_system import AdvancedCryptoSystem
from core.monitoring.intelligent_monitoring_system import IntelligentMonitoringSystem
from core.performance.intelligent_cache_system import IntelligentCacheSystem
from core.dependency.service_locator import ServiceLocator


class KworkScraper:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å–∫—Ä–µ–π–ø–µ—Ä –¥–ª—è Kwork.ru —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π:
    - –ü–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–∫–æ–ø–∏—Ä–∞–π—Ç–∏–Ω–≥, –ø–µ—Ä–µ–≤–æ–¥, —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è)
    - –§–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –±—é–¥–∂–µ—Ç—É, —Å—Ä–æ–∫–∞–º, —Ä–µ–π—Ç–∏–Ω–≥—É
    - –û–±—Ö–æ–¥–∞ Cloudflare –∏ JS-–∑–∞—â–∏—Ç—ã (—á–µ—Ä–µ–∑ —ç–º—É–ª—è—Ü–∏—é –±—Ä–∞—É–∑–µ—Ä–∞ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
    """

    def __init__(self):
        self.logger = logging.getLogger("KworkScraper")
        self.config = ServiceLocator.get("config") or UnifiedConfigManager()
        self.crypto = ServiceLocator.get("crypto") or AdvancedCryptoSystem()
        self.monitor = ServiceLocator.get("monitor") or IntelligentMonitoringSystem()
        self.cache = ServiceLocator.get("cache") or IntelligentCacheSystem()

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Kwork
        self.platform_config = self.config.get("platforms.kwork", {})
        self.base_url = self.platform_config.get("base_url", "https://kwork.ru")
        self.search_endpoint = self.platform_config.get("search_endpoint", "/projects")
        self.user_agents = self.platform_config.get("user_agents", [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15",
        ])
        self.delay_range = self.platform_config.get("delay_range", [2, 5])
        self.max_retries = self.platform_config.get("max_retries", 3)
        self.timeout = self.platform_config.get("timeout", 10)

        # –ü—Ä–æ–∫—Å–∏ (—Ä–∞—Å—à–∏—Ñ—Ä–æ–≤—ã–≤–∞—é—Ç—Å—è –∏–∑ –∑–∞—à–∏—Ñ—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞)
        encrypted_proxies = self.platform_config.get("encrypted_proxies", [])
        self.proxies = [self.crypto.decrypt(p) for p in encrypted_proxies] if encrypted_proxies else []

        self.session: Optional[aiohttp.ClientSession] = None
        self._visited_urls: Set[str] = set()

    async def __aenter__(self):
        connector = aiohttp.TCPConnector(limit=20, limit_per_host=5)
        timeout = aiohttp.ClientTimeout(total=self.timeout)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={"Accept": "text/html,application/xhtml+xml"}
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def _get_random_headers(self) -> Dict[str, str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
        return {
            "User-Agent": random.choice(self.user_agents),
            "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
            "Referer": self.base_url,
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "same-origin",
        }

    async def _fetch_page(self, url: str, use_proxy: bool = True) -> Optional[str]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç HTML-—Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ –ø—Ä–æ–∫—Å–∏."""
        for attempt in range(self.max_retries):
            try:
                headers = await self._get_random_headers()
                proxy = random.choice(self.proxies) if use_proxy and self.proxies else None

                async with self.session.get(url, headers=headers, proxy=proxy) as resp:
                    if resp.status == 200:
                        html = await resp.text()
                        self.logger.debug(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {url}")
                        return html
                    elif resp.status == 429:
                        self.logger.warning(f"‚ö†Ô∏è  Rate limited –Ω–∞ {url}, –ø–∞—É–∑–∞...")
                        await asyncio.sleep(10 * (attempt + 1))
                    elif resp.status >= 500:
                        self.logger.warning(f"‚ö†Ô∏è  –°–µ—Ä–≤–µ—Ä–Ω–∞—è –æ—à–∏–±–∫–∞ {resp.status} –Ω–∞ {url}")
                        await asyncio.sleep(5)
                    else:
                        self.logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Å—Ç–∞—Ç—É—Å {resp.status} –Ω–∞ {url}")
                        break

            except asyncio.TimeoutError:
                self.logger.warning(f"‚è≥ –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")
            except Exception as e:
                self.logger.error(f"üí• –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {e}", exc_info=True)

            await asyncio.sleep(random.uniform(*self.delay_range))

        self.monitor.log_anomaly("kwork_scraper_failure", {"url": url, "attempts": self.max_retries})
        return None

    def _parse_job_card(self, card: BeautifulSoup) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç –æ–¥–Ω—É –∫–∞—Ä—Ç–æ—á–∫—É –∑–∞–∫–∞–∑–∞."""
        try:
            title_elem = card.select_one("div.wants-card__header-title a")
            if not title_elem:
                return None

            job_id = title_elem.get("href", "").split("/")[-1]
            title = title_elem.get_text(strip=True)
            price_elem = card.select_one("div.wants-card__price span")
            price = price_elem.get_text(strip=True) if price_elem else "N/A"

            desc_elem = card.select_one("div.wants-card__description")
            description = desc_elem.get_text(strip=True) if desc_elem else ""

            deadline_elem = card.select_one("div.wants-card__right div.text-muted")
            deadline = deadline_elem.get_text(strip=True) if deadline_elem else ""

            return {
                "platform": "kwork",
                "job_id": job_id,
                "title": title,
                "description": description,
                "price_raw": price,
                "deadline_raw": deadline,
                "url": urljoin(self.base_url, title_elem["href"]),
                "scraped_at": time.time(),
                "category": self._detect_category(title, description)
            }
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏: {e}", exc_info=True)
            return None

    def _detect_category(self, title: str, description: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∑–∞–∫–∞–∑–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º."""
        text = (title + " " + description).lower()
        if any(kw in text for kw in ["—Ç—Ä–∞–Ω—Å–∫—Ä–∏–±", "—Ä–∞—Å—à–∏—Ñ—Ä–æ–≤", "–∞—É–¥–∏–æ", "–≤–∏–¥–µ–æ"]):
            return "transcription"
        elif any(kw in text for kw in ["–ø–µ—Ä–µ–≤–æ–¥", "–∞–Ω–≥–ª–∏–π—Å–∫–∏–π", "—è–∑—ã–∫", "translate"]):
            return "translation"
        elif any(kw in text for kw in ["–∫–æ–ø–∏—Ä–∞–π—Ç", "—Ç–µ–∫—Å—Ç", "—Å—Ç–∞—Ç—å—è", "seo", "—Ä–µ—Ä–∞–π—Ç"]):
            return "copywriting"
        else:
            return "other"

    async def scrape_jobs(
        self,
        categories: Optional[List[str]] = None,
        min_price: Optional[float] = None,
        max_pages: int = 5
    ) -> List[Dict[str, Any]]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥: —Å–æ–±–∏—Ä–∞–µ—Ç –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –∑–∞–∫–∞–∑—ã —Å Kwork.
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —Ü–µ–Ω–µ.
        """
        self.logger.info(f"üîç –ù–∞—á–∏–Ω–∞—é —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ Kwork (–º–∞–∫—Å. —Å—Ç—Ä–∞–Ω–∏—Ü: {max_pages})")

        if categories is None:
            categories = ["transcription", "translation", "copywriting"]

        all_jobs = []
        seen_ids = set()

        # –°–æ–±–∏—Ä–∞–µ–º URL –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        search_params = {
            "c": "1",  # —Ä–∞–∑–¥–µ–ª "–£—Å–ª—É–≥–∏"
            "sort": "new"  # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –Ω–æ–≤–∏–∑–Ω–µ
        }

        for page in range(1, max_pages + 1):
            search_params["page"] = str(page)
            query = urlencode(search_params)
            url = f"{self.base_url}{self.search_endpoint}?{query}"

            if url in self._visited_urls:
                continue
            self._visited_urls.add(url)

            html = await self._fetch_page(url)
            if not html:
                self.logger.warning(f"–ü—Ä–æ–ø—É—Å–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page} –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏")
                continue

            soup = BeautifulSoup(html, "html.parser")
            cards = soup.select("div.wants-card")

            if not cards:
                self.logger.info("üì¶ –ë–æ–ª—å—à–µ –∑–∞–∫–∞–∑–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚Äî –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ.")
                break

            for card in cards:
                job = self._parse_job_card(card)
                if not job or job["job_id"] in seen_ids:
                    continue

                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                if job["category"] not in categories:
                    continue

                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ü–µ–Ω–µ (—É–ø—Ä–æ—â—ë–Ω–Ω–æ: –∏—â–µ–º —Ü–∏—Ñ—Ä—ã)
                try:
                    price_str = job["price_raw"].replace(" ", "").replace("‚ÇΩ", "")
                    price = float(price_str) if price_str.isdigit() else 0.0
                    if min_price and price < min_price:
                        continue
                except (ValueError, AttributeError):
                    pass  # –µ—Å–ª–∏ —Ü–µ–Ω–∞ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∏–ª—å—Ç—Ä

                all_jobs.append(job)
                seen_ids.add(job["job_id"])

            self.logger.info(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –Ω–∞–π–¥–µ–Ω–æ {len(cards)} –∫–∞—Ä—Ç–æ—á–µ–∫, –≤—Å–µ–≥–æ –∑–∞–∫–∞–∑–æ–≤: {len(all_jobs)}")

            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            cache_key = f"kwork:jobs:page_{page}"
            await self.cache.set(cache_key, cards, ttl=300)

            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            await asyncio.sleep(random.uniform(*self.delay_range))

        self.logger.info(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ Kwork: –Ω–∞–π–¥–µ–Ω–æ {len(all_jobs)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∑–∞–∫–∞–∑–æ–≤")
        return all_jobs
